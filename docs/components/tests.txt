# captured_at: 2026-01-04T12:52:14.760645+00:00

# tests/architecture/test_architecture_invariants.py
from __future__ import annotations

import ast
from pathlib import Path
from typing import Iterable, List, Sequence, Tuple


REPO_ROOT = Path(__file__).resolve().parents[2]
EXCLUDED_DIRS = {".git", ".venv", "venv", "__pycache__", "storage", "secrets", "tests"}


def _iter_python_files(root: Path) -> Iterable[Path]:
    for path in root.rglob("*.py"):
        if not path.is_file():
            continue
        if any(part in EXCLUDED_DIRS for part in path.parts):
            continue
        yield path


def _read_imports(path: Path) -> List[str]:
    source = path.read_text(encoding="utf-8")
    tree = ast.parse(source, filename=str(path))
    imports: List[str] = []
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                imports.append(alias.name)
        elif isinstance(node, ast.ImportFrom):
            if node.module:
                imports.append(node.module)
    return imports


def _find_offenses(
    files: Sequence[Path],
    *,
    forbidden_prefixes: Sequence[str],
    allow_prefixes: Sequence[str] = (),
    allow_modules: Sequence[str] = (),
) -> List[Tuple[Path, str]]:
    offenses: List[Tuple[Path, str]] = []
    for path in files:
        for module in _read_imports(path):
            if module in allow_modules or any(module.startswith(prefix) for prefix in allow_prefixes):
                continue
            if any(module.startswith(prefix) for prefix in forbidden_prefixes):
                offenses.append((path, module))
    return offenses


def _format_offenses(label: str, offenses: List[Tuple[Path, str]]) -> str:
    lines = [f"{label} import violations:"]
    for path, module in sorted(offenses, key=lambda item: (str(item[0]), item[1])):
        lines.append(f"- {path}: {module}")
    return "\n".join(lines)


def test_products_do_not_import_forbidden_core_modules() -> None:
    products_root = REPO_ROOT / "products"
    files = list(_iter_python_files(products_root))
    offenses = _find_offenses(
        files,
        forbidden_prefixes=(
            "core.models",
            "core.memory",
            "core.orchestrator",
            "core.agents.llm_reasoner",
        ),
        allow_modules=("core.orchestrator.context",),
    )
    assert not offenses, _format_offenses("Products", offenses)


def test_tools_do_not_import_agents_or_models() -> None:
    tool_roots = [REPO_ROOT / "core" / "tools", REPO_ROOT / "products"]
    files: List[Path] = []
    for root in tool_roots:
        if not root.exists():
            continue
        for path in _iter_python_files(root):
            if "tools" in path.parts:
                files.append(path)
    offenses = _find_offenses(
        files,
        forbidden_prefixes=("core.agents", "core.models"),
    )
    assert not offenses, _format_offenses("Tools", offenses)


def test_agents_do_not_import_memory_backends_or_tool_executor() -> None:
    agent_roots = [REPO_ROOT / "core" / "agents", REPO_ROOT / "products"]
    files: List[Path] = []
    for root in agent_roots:
        if not root.exists():
            continue
        for path in _iter_python_files(root):
            if "agents" in path.parts:
                files.append(path)
    offenses = _find_offenses(
        files,
        forbidden_prefixes=(
            "core.memory.sqlite_backend",
            "core.memory.in_memory",
            "core.tools.executor",
        ),
    )
    assert not offenses, _format_offenses("Agents", offenses)


def test_ui_does_not_import_core_runtime_layers() -> None:
    ui_root = REPO_ROOT / "gateway" / "ui"
    files = list(_iter_python_files(ui_root))
    offenses = _find_offenses(
        files,
        forbidden_prefixes=(
            "core.orchestrator",
            "core.memory",
            "core.models",
            "core.tools",
            "core.governance",
            "core.agents",
        ),
        allow_prefixes=("core.config", "core.contracts"),
    )
    assert not offenses, _format_offenses("UI", offenses)

# tests/architecture/test_master_v1_invariants.py
from __future__ import annotations

import ast
from pathlib import Path
from typing import Iterable, List, Optional, Sequence, Tuple


REPO_ROOT = Path(__file__).resolve().parents[2]
EXCLUDED_DIRS = {".git", ".venv", "venv", "__pycache__", "storage", "secrets", "tests"}


def _iter_python_files(root: Path) -> List[Path]:
    paths: List[Path] = []
    if not root.exists():
        return paths
    for path in root.rglob("*.py"):
        if not path.is_file():
            continue
        if any(part in EXCLUDED_DIRS for part in path.parts):
            continue
        paths.append(path)
    return sorted(paths, key=lambda item: str(item))


def _read_imports(path: Path) -> List[str]:
    source = path.read_text(encoding="utf-8")
    tree = ast.parse(source, filename=str(path))
    imports: List[str] = []
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                imports.append(alias.name)
        elif isinstance(node, ast.ImportFrom):
            module = node.module
            if not module:
                continue
            for alias in node.names:
                if alias.name == "*":
                    imports.append(module)
                else:
                    imports.append(f"{module}.{alias.name}")
    return imports


def _first_violation(
    files: Sequence[Path],
    *,
    forbidden_prefixes: Sequence[str],
    forbidden_modules: Sequence[str] = (),
    allow_prefixes: Sequence[str] = (),
) -> Optional[Tuple[Path, str]]:
    for path in files:
        for module in _read_imports(path):
            if any(module.startswith(prefix) for prefix in allow_prefixes):
                continue
            if module in forbidden_modules or any(module.startswith(prefix) for prefix in forbidden_prefixes):
                return path, module
    return None


def _format_violation(label: str, violation: Optional[Tuple[Path, str]]) -> str:
    if not violation:
        return f"{label} import violation"
    path, module = violation
    rel_path = path.relative_to(REPO_ROOT)
    return f"{label} import violation: {rel_path}: {module}"


def _iter_tool_files(repo_root: Path) -> Iterable[Path]:
    tool_roots = [repo_root / "core" / "tools", repo_root / "products"]
    for root in tool_roots:
        for path in _iter_python_files(root):
            if "tools" in path.parts:
                yield path


def _iter_agent_files(repo_root: Path) -> Iterable[Path]:
    agent_roots = [repo_root / "core" / "agents", repo_root / "products"]
    for root in agent_roots:
        for path in _iter_python_files(root):
            if "agents" in path.parts:
                yield path


def test_products_do_not_import_forbidden_core_modules() -> None:
    files = _iter_python_files(REPO_ROOT / "products")
    violation = _first_violation(
        files,
        forbidden_prefixes=(
            "core.models",
            "core.memory",
            "core.orchestrator",
            "core.agents.llm_reasoner",
        ),
    )
    assert not violation, _format_violation("Products", violation)


def test_tools_do_not_import_agents_or_models() -> None:
    files = list(_iter_tool_files(REPO_ROOT))
    violation = _first_violation(
        files,
        forbidden_prefixes=("core.agents", "core.models"),
    )
    assert not violation, _format_violation("Tools", violation)


def test_agents_do_not_import_memory_backends_or_tool_executor() -> None:
    files = list(_iter_agent_files(REPO_ROOT))
    violation = _first_violation(
        files,
        forbidden_prefixes=(
            "core.memory.in_memory",
            "core.memory.sqlite_backend",
            "core.tools.executor",
        ),
    )
    assert not violation, _format_violation("Agents", violation)


def test_ui_does_not_import_core_beyond_api_surface() -> None:
    files = _iter_python_files(REPO_ROOT / "gateway" / "ui")
    violation = _first_violation(
        files,
        forbidden_prefixes=("core.",),
        allow_prefixes=("core.config", "core.contracts"),
    )
    assert not violation, _format_violation("UI", violation)

# tests/conftest.py
# ==============================
# Testing Fixtures
# ==============================
from __future__ import annotations

import sys
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import pytest
from fastapi.testclient import TestClient

ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))

from core.config.schema import Settings
from core.contracts.tool_schema import ToolError, ToolErrorCode, ToolMeta, ToolResult
from core.governance.hooks import GovernanceHooks
from core.governance.security import SecurityRedactor
from core.memory.tracing import Tracer
from core.memory.in_memory import InMemoryBackend
from core.memory.router import MemoryRouter
from core.models.providers.openai_provider import OpenAIRequest, OpenAIResponse
from core.orchestrator.engine import OrchestratorEngine
from gateway.api.http_app import create_app
from gateway.api import deps as gateway_deps


@pytest.fixture
def trace_sink() -> List[Dict[str, Any]]:
    """Collects emitted trace events without touching production logging."""
    return []


class _CollectingTracer(Tracer):
    def __init__(self, *, sink: List[Dict[str, Any]], **kwargs: Any) -> None:
        self._sink = sink
        super().__init__(**kwargs)

    def emit(self, event: Any) -> None:  # type: ignore[override]
        super().emit(event)
        payload = event.model_dump()
        if "event_type" not in payload:
            payload["event_type"] = payload.get("kind")
        self._sink.append(payload)


@pytest.fixture
def memory_backend() -> InMemoryBackend:
    """In-memory memory backend for deterministic persistence during tests."""
    return InMemoryBackend()


@pytest.fixture
def fake_model_provider() -> Callable[[OpenAIRequest], OpenAIResponse]:
    """Deterministic stub matching the OpenAI provider interface."""

    def _provider(request: OpenAIRequest) -> OpenAIResponse:
        return OpenAIResponse(
            ok=True,
            model=request.model,
            content=f"stub response for {request.model}",
            usage={"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
            meta={"stub": True},
        )

    return _provider


class FakeToolBackend:
    def __init__(self, *, behavior: str = "success") -> None:
        self.behavior = behavior
        self.calls = 0

    def run(self, tool: Any, params: Dict[str, Any], ctx: Any) -> ToolResult:
        self.calls += 1
        meta = ToolMeta(tool_name=getattr(tool, "name", tool.__class__.__name__), backend="fake")
        if self.behavior == "always_timeout":
            err = ToolError(
                code=ToolErrorCode.TIMEOUT,
                message="timeout",
                details={"params": params},
            )
            return ToolResult(ok=False, data=None, error=err, meta=meta)
        if self.behavior == "fail_once_then_success" and self.calls == 1:
            err = ToolError(
                code=ToolErrorCode.BACKEND_ERROR,
                message="simulated transient failure",
                details={"params": params},
            )
            return ToolResult(ok=False, data=None, error=err, meta=meta)
        return ToolResult.ok(data={"result": "ok"}, meta=meta)


@pytest.fixture
def fake_tool_backend() -> FakeToolBackend:
    return FakeToolBackend()


@pytest.fixture
def orchestrator(memory_backend: InMemoryBackend, trace_sink: List[Dict[str, Any]]) -> OrchestratorEngine:
    """Engine wired to deterministic in-memory helpers."""
    settings = Settings()
    memory_router = MemoryRouter(backend=memory_backend)
    tracer = _CollectingTracer(memory=memory_backend, redactor=SecurityRedactor(), sink=trace_sink)
    engine = OrchestratorEngine.from_settings(
        settings=settings,
        memory=memory_router,
        tracer=tracer,
        sleep_fn=lambda _: None,
    )
    return engine


@pytest.fixture
def app_client(orchestrator: OrchestratorEngine) -> TestClient:
    """FastAPI test client wired to the provided orchestrator."""
    gateway_deps.get_engine.cache_clear()
    gateway_deps.get_settings.cache_clear()
    gateway_deps.get_memory_router.cache_clear()
    gateway_deps.get_tracer.cache_clear()
    gateway_deps.get_product_catalog.cache_clear()
    app = create_app()
    app.dependency_overrides[gateway_deps.get_engine] = lambda: orchestrator
    client = TestClient(app)
    return client

# tests/core/test_agent_output_guardrails.py
# ==============================
# Tests: Agent Output Guardrails
# ==============================
from __future__ import annotations

import pytest

from core.agents.base import BaseAgent
from core.agents.registry import AgentRegistry
from core.config.schema import Settings
from core.contracts.agent_schema import AgentMeta, AgentResult, validate_agent_output_payload
from core.contracts.flow_schema import BackendType, StepDef, StepType
from core.governance.hooks import GovernanceHooks
from core.orchestrator.context import RunContext
from core.orchestrator.step_executor import StepExecutor


class _NoopToolExecutor:
    def execute(self, *args, **kwargs):  # type: ignore[no-untyped-def]
        raise AssertionError("tool executor should not be called")


class _ControlAgent(BaseAgent):
    name = "control_agent"

    def __init__(self, payload):  # type: ignore[no-untyped-def]
        super().__init__()
        self._payload = payload

    def run(self, step_context):  # type: ignore[no-untyped-def]
        return AgentResult(ok=True, data=self._payload, error=None, meta=AgentMeta(agent_name=self.name))


def test_agent_output_schema_rejects_control_fields() -> None:
    with pytest.raises(ValueError):
        validate_agent_output_payload({"next_step": "s2"})

    with pytest.raises(ValueError):
        validate_agent_output_payload({"data": {"branch_hint": "s3"}})


def test_step_executor_blocks_agent_control_output() -> None:
    settings = Settings()
    governance = GovernanceHooks(settings=settings)
    step_executor = StepExecutor(tool_executor=_NoopToolExecutor(), governance=governance, agent_registry=AgentRegistry)

    AgentRegistry.clear()
    try:
        AgentRegistry.register("control_agent", lambda: _ControlAgent({"next_step": "s2"}))
        run_ctx = RunContext(run_id="run_1", product="test_product", flow="test_flow")
        step_def = StepDef(
            id="step_1",
            type=StepType.AGENT,
            agent="control_agent",
            backend=BackendType.LOCAL,
        )
        with pytest.raises(RuntimeError) as exc:
            step_executor.execute(run_ctx=run_ctx, step_def=step_def)
        assert "agent_output_control_fields" in str(exc.value)
        assert "agent.control_agent.output" not in run_ctx.artifacts
    finally:
        AgentRegistry.clear()

# tests/core/test_agents_core.py
# ==============================
# Tests: Agents (Base + Registry + Minimal Agent Run)
# ==============================
from __future__ import annotations

from typing import Any, Dict, Optional

from core.agents.base import BaseAgent
from core.agents.registry import AgentRegistry
from core.contracts.agent_schema import AgentMeta, AgentResult
from core.orchestrator.context import RunContext
from core.orchestrator.state import RunStatus
from core.contracts.flow_schema import AutonomyLevel


class _EchoAgent(BaseAgent):
    def __init__(self, name: str = "echo_agent") -> None:
        self._name = name

    @property
    def name(self) -> str:
        return self._name

    def run(self, step_ctx: Any) -> AgentResult:
        # Minimal agent that echoes payload + prior artifacts
        message = None
        try:
            message = step_ctx.run.payload.get("keyword")  # type: ignore[attr-defined]
        except Exception:
            message = None

        data = {
            "echo": message,
            "artifact_keys": sorted(list(getattr(step_ctx.run, "artifacts", {}).keys())),
        }
        return AgentResult(ok=True, data=data, error=None, meta=AgentMeta(agent_name=self.name))


def test_agent_registry_register_and_resolve() -> None:
    AgentRegistry.clear()
    AgentRegistry.register("echo_agent", lambda: _EchoAgent())
    resolved = AgentRegistry.resolve("echo_agent")
    assert resolved is not None
    assert resolved.name == "echo_agent"


def test_agent_registry_duplicate_registration_raises() -> None:
    AgentRegistry.clear()
    AgentRegistry.register("dup_agent", lambda: _EchoAgent("dup_agent"))
    try:
        AgentRegistry.register("dup_agent", lambda: _EchoAgent("dup_agent"))
        assert False, "Expected duplicate registration to raise"
    except ValueError:
        assert True


def test_agent_run_returns_agent_result() -> None:
    AgentRegistry.clear()
    AgentRegistry.register("echo_agent", lambda: _EchoAgent())

    run = RunContext(
        run_id="r1",
        product="hello_world",
        flow="hello_world",
        status=RunStatus.RUNNING,
        payload={"keyword": "hi"},
        artifacts={"k1": {"v": 1}},
        meta={},
    )
    step = run.new_step(step_id="s_agent", step_type="agent", backend="local", target="echo_agent")

    resolved = AgentRegistry.resolve("echo_agent")
    assert resolved is not None

    res = resolved.run(step)
    assert res.ok is True
    assert res.data is not None
    assert res.data["echo"] == "hi"
    assert "k1" in res.data["artifact_keys"]

# tests/core/test_concurrency_smoke.py
# ==============================
# Tests: Concurrency Smoke
# ==============================
from __future__ import annotations

from concurrent.futures import ThreadPoolExecutor
from pathlib import Path

from core.agents.registry import AgentRegistry
from core.config.schema import Settings
from core.contracts.tool_schema import ToolMeta, ToolResult
from core.governance.hooks import GovernanceHooks
from core.governance.security import SecurityRedactor
from core.memory.in_memory import InMemoryBackend
from core.memory.router import MemoryRouter
from core.memory.tracing import Tracer
from core.orchestrator.engine import OrchestratorEngine
from core.orchestrator.flow_loader import FlowLoader
from core.orchestrator.step_executor import StepExecutor
from core.tools.base import BaseTool
from core.tools.executor import ToolExecutor
from core.tools.registry import ToolRegistry


class _RunIdTool(BaseTool):
    name = "run_id_tool"

    def run(self, params, ctx):  # type: ignore[no-untyped-def]
        meta = ToolMeta(tool_name=self.name, backend="local")
        payload = {"summary": ctx.run_id, "details": {"run_id": ctx.run_id, "marker": params.get("marker")}}
        return ToolResult.ok(data=payload, meta=meta)


def _write_flow(tmp_path: Path) -> Path:
    flows_dir = tmp_path / "products" / "test_product" / "flows"
    flows_dir.mkdir(parents=True, exist_ok=True)
    flow_path = flows_dir / "test_flow.yaml"
    flow_path.write_text(
        "\n".join(
            [
                'id: "test_flow"',
                'version: "1.0.0"',
                "steps:",
                '  - id: "run_tool"',
                '    type: "tool"',
                '    backend: "local"',
                '    tool: "run_id_tool"',
                "    params:",
                '      marker: "{{payload.marker}}"',
                "",
            ]
        ),
        encoding="utf-8",
    )
    return flow_path


def _build_engine(tmp_path: Path) -> OrchestratorEngine:
    flow_path = _write_flow(tmp_path)
    flow_loader = FlowLoader(products_root=flow_path.parents[2])
    memory = MemoryRouter(backend=InMemoryBackend())
    tracer = Tracer(memory=memory, mirror_to_log=False)
    settings = Settings()
    governance = GovernanceHooks(settings=settings)
    tool_executor = ToolExecutor(registry=ToolRegistry, hooks=governance, redactor=SecurityRedactor())
    step_executor = StepExecutor(tool_executor=tool_executor, governance=governance, agent_registry=AgentRegistry)
    return OrchestratorEngine(
        flow_loader=flow_loader,
        step_executor=step_executor,
        memory=memory,
        tracer=tracer,
        governance=governance,
    )


def test_parallel_runs_isolated(tmp_path: Path) -> None:
    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        ToolRegistry.register("run_id_tool", lambda: _RunIdTool())
        engine = _build_engine(tmp_path)

        def _run(marker: str):
            res = engine.run_flow(product="test_product", flow="test_flow", payload={"marker": marker})
            assert res.ok
            return res.data["run_id"], marker

        with ThreadPoolExecutor(max_workers=2) as executor:
            futures = [executor.submit(_run, "a"), executor.submit(_run, "b")]
            results = [f.result() for f in futures]

        for run_id, marker in results:
            bundle = engine.memory.get_run(run_id)
            assert bundle is not None
            output = bundle.run.output or {}
            assert output.get("run_id") == run_id
            assert output.get("marker") == marker
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()

# tests/core/test_config_precedence_extended.py
# ==============================
# Config Precedence Tests
# ==============================
from __future__ import annotations

import textwrap

import pytest

from core.config.loader import load_settings


def _write_yaml(path, body: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(textwrap.dedent(body), encoding="utf-8")


def _base_configs(root):
    _write_yaml(root / "configs" / "app.yaml", """\
    app:
      host: config-host
      port: 1111
      paths:
        storage_dir: storage
    """)
    _write_yaml(root / "configs" / "models.yaml", """\
    models:
      openai:
        timeout_seconds: 5.0
    """)
    _write_yaml(root / "configs" / "policies.yaml", "policies: {}\n")
    _write_yaml(root / "configs" / "logging.yaml", "logging: {}\n")
    _write_yaml(root / "configs" / "products.yaml", "products: {}\n")


def test_config_precedence(monkeypatch, tmp_path):
    repo_root = tmp_path / "repo"
    repo_root.mkdir()
    _base_configs(repo_root)

    secrets_dir = repo_root / "secrets"
    secrets_dir.mkdir()
    secrets_path = secrets_dir / "secrets.yaml"
    _write_yaml(secrets_path, """\
    secrets:
      openai_api_key: secret-key
    """)

    env = {"MASTER__APP__PORT": "3333"}

    settings = load_settings(
        repo_root=str(repo_root),
        configs_dir="configs",
        secrets_path=str(secrets_path),
        env=env,
    )

    assert settings.app.port == 3333
    assert settings.models.openai.api_key == "secret-key"

    # Negative case: invalid products config should raise early
    _write_yaml(repo_root / "bad_configs" / "app.yaml", """\
    app:
      env: local
      paths:
        repo_root: .
      port: not-a-number
    """)
    with pytest.raises(ValueError) as excinfo:
        load_settings(
            repo_root=str(repo_root),
            configs_dir="bad_configs",
            secrets_path=str(secrets_path),
        )
    assert "Invalid configuration" in str(excinfo.value)

# tests/core/test_contracts.py
# ==============================
# Tests: Core Contracts
# ==============================
from __future__ import annotations

from typing import Any, Dict

import pytest
from pydantic import ValidationError

from core.contracts.agent_schema import AgentError, AgentMeta, AgentResult
from core.contracts.flow_schema import AutonomyLevel, FlowDef, StepDef, StepType
from core.contracts.run_schema import ArtifactRef, RunRecord, StepRecord, TraceEvent
from core.contracts.tool_schema import ToolError, ToolMeta, ToolResult


def test_tool_result_envelope_ok() -> None:
    r = ToolResult(ok=True, data={"x": 1}, error=None, meta=ToolMeta(tool_name="t", backend="local"))
    assert r.ok is True
    assert r.data == {"x": 1}
    assert r.error is None
    assert r.meta.tool_name == "t"


def test_tool_result_requires_error_when_not_ok() -> None:
    with pytest.raises(ValidationError):
        ToolResult(ok=False, data=None, error=None, meta=ToolMeta(tool_name="t", backend="local"))


def test_agent_result_envelope_ok() -> None:
    r = AgentResult(ok=True, data={"y": 2}, error=None, meta=AgentMeta(agent_name="a"))
    assert r.ok is True
    assert r.data == {"y": 2}
    assert r.error is None


def test_agent_result_requires_error_when_not_ok() -> None:
    with pytest.raises(ValidationError):
        AgentResult(ok=False, data=None, error=None, meta=AgentMeta(agent_name="a"))


def test_flow_def_and_steps_validate() -> None:
    f = FlowDef(
        name="hello",
        version="1.0.0",
        autonomy_level=AutonomyLevel.suggest_only,
        steps=[
            StepDef(id="s1", type=StepType.tool, tool="echo_tool", backend="local"),
            StepDef(id="s2", type=StepType.human_approval, title="Approve", message="ok?"),
            StepDef(id="s3", type=StepType.agent, agent="simple_agent", backend="local"),
        ],
    )
    assert f.name == "hello"
    assert f.steps[0].type == StepType.tool


def test_step_def_requires_tool_or_agent() -> None:
    with pytest.raises(ValidationError):
        StepDef(id="bad", type=StepType.tool, backend="local")

    with pytest.raises(ValidationError):
        StepDef(id="bad2", type=StepType.agent, backend="local")


def test_run_models_validate_minimal() -> None:
    run = RunRecord(run_id="r1", product="hello_world", flow="hello_world", status="RUNNING")
    step = StepRecord(run_id="r1", step_id="s1", status="RUNNING")
    event = TraceEvent(run_id="r1", step_id="s1", product="hello_world", flow="hello_world", event_type="test", payload={"a": 1})
    art = ArtifactRef(key="k", kind="json", uri="memory://k")
    assert run.run_id == "r1"
    assert step.step_id == "s1"
    assert event.payload["a"] == 1
    assert art.key == "k"

# tests/core/test_dotenv_loading.py
# ==============================
# .env Loading Tests
# ==============================
from __future__ import annotations

import textwrap

from core.config.loader import load_settings


def _write_yaml(path, body: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(textwrap.dedent(body), encoding="utf-8")


def _base_configs(root) -> None:
    _write_yaml(root / "configs" / "app.yaml", """\
    app:
      env: local
      paths:
        storage_dir: storage
    """)
    _write_yaml(root / "configs" / "models.yaml", "models: {}\n")
    _write_yaml(root / "configs" / "policies.yaml", "policies: {}\n")
    _write_yaml(root / "configs" / "logging.yaml", "logging: {}\n")
    _write_yaml(root / "configs" / "products.yaml", "products: {}\n")


def test_loads_dotenv_and_resolves_secrets(monkeypatch, tmp_path) -> None:
    repo_root = tmp_path / "repo"
    repo_root.mkdir()
    _base_configs(repo_root)

    secrets_dir = repo_root / "secrets"
    secrets_dir.mkdir()
    _write_yaml(secrets_dir / "secrets.yaml", """\
    secrets:
      openai:
        api_key: test-openai-key
    """)

    dotenv_path = tmp_path / ".env"
    dotenv_path.write_text(
        "APP_BASE_PATH=repo\nOPENAI_API_KEY_REF=openai.api_key\n",
        encoding="utf-8",
    )

    monkeypatch.chdir(tmp_path)
    settings = load_settings(env={})

    assert settings.models.openai.api_key == "test-openai-key"

# tests/core/test_governance_core.py
from __future__ import annotations

from typing import Dict, List

from core.config.schema import Settings
from core.contracts.flow_schema import AutonomyLevel, FlowDef, StepDef, StepType
from core.contracts.run_schema import RunRecord, RunStatus
from core.governance.policies import PolicyEngine
from core.governance.security import SecurityRedactor
from core.orchestrator.context import RunContext, StepContext


def _settings() -> Settings:
    return Settings()


def _step_ctx(product: str = "hello_world") -> StepContext:
    run_record = RunRecord(run_id="run_test", product=product, flow_id="hello", status=RunStatus.RUNNING)
    flow = FlowDef(
        id="hello",
        steps=[StepDef(id="s1", type=StepType.TOOL, tool="echo_tool", backend=None)],
    )
    run_ctx = RunContext(run_id=run_record.run_id, product=run_record.product, flow=flow.id)
    return run_ctx.new_step(step_def=flow.steps[0])


def test_redactor_scrubs_tokens_and_pii() -> None:
    redactor = SecurityRedactor()
    payload = {
        "api_key": "sk-12345678901234567890ABCDE",
        "nested": {"Authorization": "Bearer sk-AAAAAAAAAAAAAAAAAAAAAAAAAA"},
        "contact": "user@example.com",
    }
    sanitized = redactor.sanitize(payload)
    assert sanitized["api_key"] == SecurityRedactor().mask
    assert "sk-" not in str(sanitized["nested"]["Authorization"])
    assert "user@example.com" not in sanitized["contact"]


def test_policy_engine_allows_tool_by_default() -> None:
    settings = _settings()
    engine = PolicyEngine(settings)
    decision = engine.evaluate_tool_call(tool_name="echo_tool", step_ctx=_step_ctx())
    assert decision.allow is True


def test_policy_engine_blocks_tool_via_blocklist() -> None:
    settings = _settings()
    settings.policies.blocked_tools = ["echo_tool"]
    engine = PolicyEngine(settings)
    decision = engine.evaluate_tool_call(tool_name="echo_tool", step_ctx=_step_ctx())
    assert decision.allow is False
    assert decision.reason == "tool_blocked"


def test_policy_engine_applies_per_product_override() -> None:
    settings = _settings()
    settings.policies.blocked_tools = ["echo_tool"]
    settings.policies.by_product = {"hello_world": {"blocked_tools": []}}
    engine = PolicyEngine(settings)
    decision = engine.evaluate_tool_call(tool_name="echo_tool", step_ctx=_step_ctx())
    assert decision.allow is True


def test_policy_engine_enforces_model_allowlist() -> None:
    settings = _settings()
    settings.policies.allowed_models = ["gpt-4o-mini"]
    engine = PolicyEngine(settings)
    allowed = engine.evaluate_model_selection(product="hello_world", model_name="gpt-4o-mini")
    denied = engine.evaluate_model_selection(product="hello_world", model_name="other-model")
    assert allowed.allow is True
    assert denied.allow is False

# tests/core/test_governance_limits.py
# ==============================
# Governance Limits Tests
# ==============================
from __future__ import annotations

from pathlib import Path

from core.config.schema import AppConfig, PathsConfig, PoliciesConfig, Settings
from core.contracts.reasoning_schema import ReasoningPurpose
from core.governance.hooks import GovernanceHooks
from core.memory.in_memory import InMemoryBackend
from core.memory.router import MemoryRouter
from core.memory.tracing import Tracer
from core.orchestrator.context import RunContext
from core.orchestrator.engine import OrchestratorEngine


def _settings_with_policies(repo_root: Path, *, policies: PoliciesConfig) -> Settings:
    app = AppConfig(paths=PathsConfig(repo_root=repo_root.as_posix()))
    return Settings(app=app, policies=policies)


def test_max_payload_bytes_exceeded(tmp_path) -> None:
    repo_root = Path(__file__).resolve().parents[2]
    policies = PoliciesConfig(max_payload_bytes=10)
    settings = _settings_with_policies(repo_root, policies=policies)
    memory = MemoryRouter(backend=InMemoryBackend())
    tracer = Tracer(memory=memory)
    engine = OrchestratorEngine.from_settings(settings=settings, memory=memory, tracer=tracer, sleep_fn=lambda _: None)

    res = engine.run_flow(product="hello_world", flow="hello_world", payload={"keyword": "too-large-payload"})
    assert res.ok is False
    assert res.error is not None
    assert res.error.code == "payload_limit_exceeded"

    runs = memory.list_runs()
    assert runs
    assert runs[0].status == "FAILED"
    assert runs[0].summary.get("error", {}).get("code") == "payload_limit_exceeded"


def test_max_steps_exceeded(tmp_path) -> None:
    repo_root = Path(__file__).resolve().parents[2]
    policies = PoliciesConfig(max_steps=1)
    settings = _settings_with_policies(repo_root, policies=policies)
    memory = MemoryRouter(backend=InMemoryBackend())
    tracer = Tracer(memory=memory)
    engine = OrchestratorEngine.from_settings(settings=settings, memory=memory, tracer=tracer, sleep_fn=lambda _: None)

    res = engine.run_flow(product="hello_world", flow="hello_world", payload={"keyword": "hello"})
    assert res.ok is False
    assert res.error is not None
    assert res.error.code == "max_steps_exceeded"

    runs = memory.list_runs()
    assert runs
    assert runs[0].status == "FAILED"
    assert runs[0].summary.get("error", {}).get("code") == "max_steps_exceeded"


def test_max_tool_calls_exceeded() -> None:
    policies = PoliciesConfig(max_tool_calls=1)
    settings = Settings(policies=policies)
    hooks = GovernanceHooks(settings=settings)
    run_ctx = RunContext(run_id="run_1", product="demo", flow="demo", payload={})
    step_ctx = run_ctx.new_step(step_id="tool_step", step_type="tool")

    decision1 = hooks.before_tool_call(tool_name="tool_a", params={}, ctx=step_ctx)
    assert decision1.allowed is True

    decision2 = hooks.before_tool_call(tool_name="tool_b", params={}, ctx=step_ctx)
    assert decision2.allowed is False
    assert decision2.reason == "tool_call_limit_exceeded"


def test_max_tokens_exceeded() -> None:
    policies = PoliciesConfig(model_max_tokens=5)
    settings = Settings(policies=policies)
    hooks = GovernanceHooks(settings=settings)
    run_ctx = RunContext(run_id="run_1", product="demo", flow="demo", payload={})
    step_ctx = run_ctx.new_step(step_id="agent_step", step_type="agent")

    decision = hooks.before_model_call(
        model_name="gpt-4o-mini",
        purpose=ReasoningPurpose.EXPLANATION,
        messages={"messages": [{"role": "user", "content": "hi"}]},
        max_tokens=10,
        ctx=step_ctx,
    )
    assert decision.allowed is False
    assert decision.reason == "model_token_limit_exceeded"


def test_max_steps_per_run_exceeded() -> None:
    policies = PoliciesConfig(max_steps=1)
    settings = Settings(policies=policies)
    hooks = GovernanceHooks(settings=settings)
    run_ctx = RunContext(run_id="run_1", product="demo", flow="demo", payload={})
    run_ctx.meta["steps_executed"] = 1
    step_ctx = run_ctx.new_step(step_id="agent_step", step_type="agent")

    decision = hooks.before_step(step_ctx=step_ctx)
    assert decision.allowed is False
    assert decision.reason == "step_limit_exceeded"


def test_run_token_budget_exceeded() -> None:
    policies = PoliciesConfig(max_tokens_per_run=5)
    settings = Settings(policies=policies)
    hooks = GovernanceHooks(settings=settings)
    run_ctx = RunContext(run_id="run_1", product="demo", flow="demo", payload={})
    run_ctx.meta["tokens_used"] = 4
    step_ctx = run_ctx.new_step(step_id="agent_step", step_type="agent")

    decision = hooks.before_model_call(
        model_name="gpt-4o-mini",
        purpose=ReasoningPurpose.EXPLANATION,
        messages={"messages": [{"role": "user", "content": "hi"}]},
        max_tokens=2,
        ctx=step_ctx,
    )
    assert decision.allowed is False
    assert decision.reason == "run_token_budget_exceeded"

# tests/core/test_guardrails_repo_scan.py
# ==============================
# Guardrail Validations
# ==============================
from __future__ import annotations

import pathlib
import re
from typing import List


REPO_ROOT = pathlib.Path(__file__).resolve().parents[2]
EXCLUDED_DIRS = {".git", ".venv", "venv", "__pycache__", "storage", "secrets", "tests", "scripts"}


def _iter_python_files() -> List[pathlib.Path]:
    files: List[pathlib.Path] = []
    for path in REPO_ROOT.rglob("*.py"):
        if any(part in EXCLUDED_DIRS for part in path.parts):
            continue
        files.append(path)
    return files


def _read(path: pathlib.Path) -> str:
    return path.read_text(encoding="utf-8")


def _find_offenses(pattern: str, files: List[pathlib.Path], *, allow: List[pathlib.Path]) -> List[pathlib.Path]:
    offenders: List[pathlib.Path] = []
    for path in files:
        if any(path == allowed or allowed in path.parents for allowed in allow):
            continue
        if re.search(pattern, _read(path)):
            offenders.append(path)
    return offenders


def test_no_os_environ_outside_loader() -> None:
    files = _iter_python_files()
    permitted = [REPO_ROOT / "core" / "config" / "loader.py"]
    pattern = r"os\.environ"
    offenders = _find_offenses(pattern, files, allow=permitted)
    assert not offenders, f"os.environ reads only allowed in core/config/loader.py, found in: {offenders}"


def test_model_router_is_only_provider_entrypoint() -> None:
    files = _iter_python_files()
    permitted = [
        REPO_ROOT / "core" / "models" / "router.py",
        REPO_ROOT / "core" / "models" / "providers",
    ]
    pattern = r"\bcore\.models\b"
    offenders = _find_offenses(pattern, files, allow=permitted)
    assert not offenders, (
        "Direct model/provider imports are only permitted under core/models/router.py "
        f"or core/models/providers/*, found in: {offenders}"
    )


def test_tool_executor_is_centralized() -> None:
    files = _iter_python_files()
    allowed = {
        REPO_ROOT / "core" / "orchestrator" / "engine.py",
        REPO_ROOT / "core" / "orchestrator" / "step_executor.py",
        REPO_ROOT / "tests" / "core" / "test_tools_core.py",
        REPO_ROOT / "core" / "tools" / "executor.py",
    }
    pattern = r"ToolExecutor"
    offenders = _find_offenses(pattern, files, allow=list(allowed))
    assert not offenders, (
        "Only orchestrator/engine, orchestrator/step_executor, and test_tools_core should reference ToolExecutor directly. "
        f"Offenders: {offenders}"
    )


def test_sqlite_imports_constrained_to_persistence_modules() -> None:
    files = _iter_python_files()
    allowed = {
        REPO_ROOT / "core" / "memory" / "sqlite_backend.py",
    }
    pattern = r"\bsqlite3\b"
    offenders = _find_offenses(pattern, files, allow=list(allowed))
    assert not offenders, (
        "Direct sqlite3 usage must be contained within persistence helpers. Offending files: {offenders}"
    )

# tests/core/test_hf_storage_remap.py
# ==============================
# Hugging Face Storage Remap Tests
# ==============================
from __future__ import annotations

import textwrap

from core.config.loader import load_settings


def _write_yaml(path, body: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(textwrap.dedent(body), encoding="utf-8")


def _base_configs(root) -> None:
    _write_yaml(root / "configs" / "app.yaml", "app: {}\n")
    _write_yaml(root / "configs" / "models.yaml", "models: {}\n")
    _write_yaml(root / "configs" / "policies.yaml", "policies: {}\n")
    _write_yaml(root / "configs" / "logging.yaml", "logging: {}\n")
    _write_yaml(root / "configs" / "products.yaml", "products: {}\n")


def test_hf_env_applies_persistent_paths_when_unset(tmp_path) -> None:
    repo_root = tmp_path / "repo"
    repo_root.mkdir()
    _base_configs(repo_root)

    settings = load_settings(
        repo_root=str(repo_root),
        configs_dir="configs",
        env={"HF_HOME": "/data"},
    )

    assert settings.app.paths.storage_dir == "/data/storage"
    assert settings.app.paths.observability_dir == "/data/observability"


def test_hf_env_respects_env_overrides(tmp_path) -> None:
    repo_root = tmp_path / "repo"
    repo_root.mkdir()
    _base_configs(repo_root)

    settings = load_settings(
        repo_root=str(repo_root),
        configs_dir="configs",
        env={
            "HF_HOME": "/data",
            "MASTER__APP__PATHS__STORAGE_DIR": "/custom/storage",
            "MASTER__APP__PATHS__OBSERVABILITY_DIR": "/custom/observability",
        },
    )

    assert settings.app.paths.storage_dir == "/custom/storage"
    assert settings.app.paths.observability_dir == "/custom/observability"

# tests/core/test_hitl_idempotency.py
# ==============================
# HITL Idempotency Tests
# ==============================
from __future__ import annotations

from typing import List

import pytest

from core.agents.registry import AgentRegistry
from core.config.loader import load_settings
from core.contracts.run_schema import RunStatus
from core.memory.router import MemoryRouter
from core.tools.registry import ToolRegistry
from core.utils.product_loader import discover_products, register_enabled_products


def _register_products():
    settings = load_settings()
    AgentRegistry.clear()
    ToolRegistry.clear()
    catalog = discover_products(settings)
    register_enabled_products(catalog, settings=settings)
    return settings


def test_hitl_idempotency(orchestrator, trace_sink: List[dict]) -> None:
    settings = _register_products()
    trace_sink.clear()

    # RunA -> Approve, double approve should fail
    start = orchestrator.run_flow(product="hello_world", flow="hello_world", payload={"keyword": "idempotent"})
    assert start.ok
    run_id = start.data["run_id"]
    bundle = orchestrator.memory.get_run(run_id)
    assert bundle and bundle.run.status == RunStatus.PENDING_HUMAN

    # Persisted state accessible through new router
    router = MemoryRouter(backend=orchestrator.memory.backend)
    fresh = router.get_run(run_id)
    assert fresh and fresh.run.status == RunStatus.PENDING_HUMAN

    ok = orchestrator.resume_run(run_id=run_id, approval_payload={"approved": True}, decision="APPROVED")
    assert ok.ok
    result = orchestrator.resume_run(run_id=run_id, approval_payload={"approved": True}, decision="APPROVED")
    assert not result.ok
    assert result.error and result.error.code == "invalid_state"

    trace_types = [event["kind"] for event in trace_sink]
    assert "pending_human" in trace_types
    assert "run_resumed" in trace_types
    assert "run_completed" in trace_types

    trace_sink.clear()

    # RunB -> Reject, double reject and approve-after-reject should fail
    start_b = orchestrator.run_flow(product="hello_world", flow="hello_world", payload={"keyword": "idempotent"})
    assert start_b.ok
    run_b = start_b.data["run_id"]
    reject_ok = orchestrator.resume_run(run_id=run_b, approval_payload={"approved": False}, decision="REJECTED")
    assert reject_ok.ok

    second_reject = orchestrator.resume_run(run_id=run_b, approval_payload={"approved": False}, decision="REJECTED")
    assert not second_reject.ok
    assert second_reject.error and second_reject.error.code == "invalid_state"

    approve_after_reject = orchestrator.resume_run(run_id=run_b, approval_payload={"approved": True}, decision="APPROVED")
    assert not approve_after_reject.ok
    assert approve_after_reject.error and approve_after_reject.error.code == "invalid_state"

# tests/core/test_log_redaction.py
# ==============================
# Log Redaction Tests
# ==============================
from __future__ import annotations

import logging

from core.agents.registry import AgentRegistry
from core.config.loader import load_settings
from core.governance.security import DEFAULT_MASK
from core.tools.registry import ToolRegistry
from core.utils.product_loader import discover_products, register_enabled_products


def _register_products() -> None:
    settings = load_settings()
    AgentRegistry.clear()
    ToolRegistry.clear()
    catalog = discover_products(settings)
    register_enabled_products(catalog, settings=settings)


def test_log_redaction(orchestrator, trace_sink, caplog) -> None:
    _register_products()
    secret_value = "sk-very-secret TOKEN=pa55word"
    caplog.set_level(logging.INFO, logger="master.trace")

    orchestrator.run_flow(product="hello_world", flow="hello_world", payload={"keyword": secret_value})

    assert secret_value not in caplog.text
    for record in caplog.records:
        message = record.getMessage()
        assert secret_value not in message
        assert secret_value not in str(record.__dict__)

    assert trace_sink, "Expected traces recorded"
    for event in trace_sink:
        payload_repr = str(event.get("payload", {}))
        assert secret_value not in payload_repr

# tests/core/test_openai_provider_headers.py
# ==============================
# OpenAI Provider Header Tests
# ==============================
from __future__ import annotations

from core.models.providers.openai_provider import _should_send_org_header


def test_org_header_skips_placeholder() -> None:
    assert _should_send_org_header("PUT_OPENAI_ORG_ID_HERE") is False
    assert _should_send_org_header("placeholder") is False
    assert _should_send_org_header("YOUR_ORG_ID") is False
    assert _should_send_org_header("  ") is False


def test_org_header_allows_realistic_value() -> None:
    assert _should_send_org_header("org_1234567890") is True

# tests/core/test_orchestrator.py
# ==============================
# Tests: Orchestrator (Engine Basic + HITL Pause/Resume)
# ==============================
from __future__ import annotations

from pathlib import Path

import pytest

from core.config.loader import load_settings
from core.utils.product_loader import discover_products, register_enabled_products
from core.orchestrator.engine import OrchestratorEngine
from core.agents.registry import AgentRegistry
from core.tools.registry import ToolRegistry


@pytest.mark.integration
def test_engine_runs_and_pauses_on_hitl(tmp_path: Path) -> None:
    repo_root = Path(__file__).resolve().parents[2]
    configs_dir = repo_root / "configs"
    secrets_dir = tmp_path / "secrets"
    secrets_dir.mkdir(parents=True, exist_ok=True)
    secrets_path = secrets_dir / "secrets.yaml"
    sqlite_path = tmp_path / "engine_test.sqlite"

    secrets_path.write_text(
        "\n".join(
            [
                "secrets:",
                "  db:",
                f"    sqlite_path: '{sqlite_path.as_posix()}'",
                "",
            ]
        ),
        encoding="utf-8",
    )

    settings = load_settings(configs_dir=str(configs_dir), secrets_path=str(secrets_path))

    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        catalog = discover_products(settings, repo_root=repo_root)
        register_enabled_products(catalog, settings=settings)

        engine = OrchestratorEngine.from_settings(settings)

        started = engine.run_flow(product="hello_world", flow="hello_world", payload={"keyword": "test"})
        assert started.ok, started.error
        run_id = started.data["run_id"]  # type: ignore[index]

        status = engine.get_run(run_id=run_id)
        assert status.ok, status.error
        assert status.data and status.data["run"]["status"] in ("PENDING_HUMAN", "pending_human")

        resumed = engine.resume_run(run_id=run_id, approval_payload={"approved": True, "notes": "ok"})
        assert resumed.ok, resumed.error

        status2 = engine.get_run(run_id=run_id)
        assert status2.ok, status2.error
        assert status2.data and status2.data["run"]["status"] in ("COMPLETED", "completed")
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()

# tests/core/test_orchestrator_state.py
# ==============================
# Tests: Orchestrator State Enums + Transitions
# ==============================
from __future__ import annotations

from core.orchestrator.state import RunStatus, StepStatus, RunState, is_valid_run_transition, to_run_state


def test_run_status_has_pending_human() -> None:
    assert hasattr(RunStatus, "PENDING_HUMAN")
    assert RunStatus.PENDING_HUMAN.value == "PENDING_HUMAN"
    assert hasattr(RunStatus, "PENDING_USER_INPUT")
    assert RunStatus.PENDING_USER_INPUT.value == "PENDING_USER_INPUT"


def test_step_status_has_pending_human() -> None:
    assert hasattr(StepStatus, "PENDING_HUMAN")
    assert StepStatus.PENDING_HUMAN.value == "PENDING_HUMAN"
    assert hasattr(StepStatus, "PENDING_USER_INPUT")
    assert StepStatus.PENDING_USER_INPUT.value == "PENDING_USER_INPUT"


def test_status_string_roundtrip() -> None:
    # Ensure enum values are stable strings (used in DB/status API).
    assert str(RunStatus.RUNNING.value) == "RUNNING"
    assert str(StepStatus.COMPLETED.value) == "COMPLETED"


def test_run_state_mapping() -> None:
    assert to_run_state(RunStatus.PENDING_HUMAN) == RunState.PENDING_APPROVAL
    assert to_run_state(RunStatus.PENDING_USER_INPUT) == RunState.PENDING_USER_INPUT


def test_run_state_transitions() -> None:
    assert is_valid_run_transition(RunStatus.RUNNING, RunStatus.PENDING_HUMAN)
    assert is_valid_run_transition(RunStatus.PENDING_HUMAN, RunStatus.RUNNING)
    assert is_valid_run_transition(RunStatus.PENDING_USER_INPUT, RunStatus.RUNNING)
    assert not is_valid_run_transition(RunStatus.COMPLETED, RunStatus.RUNNING)

# tests/core/test_output_governance.py
# ==============================
# Tests: Output Governance Enforcement
# ==============================
from __future__ import annotations

from pathlib import Path

from core.agents.registry import AgentRegistry
from core.config.schema import PoliciesConfig, Settings
from core.contracts.run_schema import RunStatus
from core.contracts.tool_schema import ToolMeta, ToolResult
from core.governance.hooks import GovernanceHooks
from core.governance.security import SecurityRedactor
from core.memory.in_memory import InMemoryBackend
from core.memory.router import MemoryRouter
from core.memory.tracing import Tracer
from core.orchestrator.engine import OrchestratorEngine
from core.orchestrator.flow_loader import FlowLoader
from core.orchestrator.step_executor import StepExecutor
from core.tools.base import BaseTool
from core.tools.executor import ToolExecutor
from core.tools.registry import ToolRegistry


class _BigOutputTool(BaseTool):
    name = "big_output_tool"

    def run(self, params, ctx):  # type: ignore[no-untyped-def]
        meta = ToolMeta(tool_name=self.name, backend="local")
        return ToolResult.ok(data={"summary": "x" * 200, "details": {"ok": True}}, meta=meta)


class _OutputFilesTool(BaseTool):
    name = "output_files_tool"

    def run(self, params, ctx):  # type: ignore[no-untyped-def]
        meta = ToolMeta(tool_name=self.name, backend="local")
        return ToolResult.ok(
            data={
                "summary": "ok",
                "details": {"ok": True},
                "output_files": [{"name": "big.txt", "content_base64": "x" * 200}],
            },
            meta=meta,
        )


def _write_flow(tmp_path: Path, *, tool_name: str) -> Path:
    flows_dir = tmp_path / "products" / "test_product" / "flows"
    flows_dir.mkdir(parents=True, exist_ok=True)
    flow_path = flows_dir / "test_flow.yaml"
    flow_path.write_text(
        "\n".join(
            [
                'id: "test_flow"',
                'version: "1.0.0"',
                "steps:",
                '  - id: "run_tool"',
                '    type: "tool"',
                '    backend: "local"',
                f'    tool: "{tool_name}"',
                "    params: {}",
                "",
            ]
        ),
        encoding="utf-8",
    )
    return flow_path


def _build_engine(tmp_path: Path, *, settings: Settings, tool_name: str) -> OrchestratorEngine:
    flow_path = _write_flow(tmp_path, tool_name=tool_name)
    flow_loader = FlowLoader(products_root=flow_path.parents[2])
    memory = MemoryRouter(backend=InMemoryBackend())
    tracer = Tracer(memory=memory, mirror_to_log=False)
    governance = GovernanceHooks(settings=settings)
    tool_executor = ToolExecutor(registry=ToolRegistry, hooks=governance, redactor=SecurityRedactor())
    step_executor = StepExecutor(tool_executor=tool_executor, governance=governance, agent_registry=AgentRegistry)
    return OrchestratorEngine(
        flow_loader=flow_loader,
        step_executor=step_executor,
        memory=memory,
        tracer=tracer,
        governance=governance,
    )


def test_output_payload_limit_blocks_run(tmp_path: Path) -> None:
    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        ToolRegistry.register("big_output_tool", lambda: _BigOutputTool())
        settings = Settings(policies=PoliciesConfig(max_payload_bytes=50))
        engine = _build_engine(tmp_path, settings=settings, tool_name="big_output_tool")

        res = engine.run_flow(product="test_product", flow="test_flow", payload={})
        assert res.ok
        assert res.data["status"] == RunStatus.FAILED.value
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()


def test_output_files_limit_blocks_run(tmp_path: Path) -> None:
    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        ToolRegistry.register("output_files_tool", lambda: _OutputFilesTool())
        settings = Settings(policies=PoliciesConfig(max_payload_bytes=50))
        engine = _build_engine(tmp_path, settings=settings, tool_name="output_files_tool")

        res = engine.run_flow(product="test_product", flow="test_flow", payload={})
        assert res.ok
        assert res.data["status"] == RunStatus.FAILED.value
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()

# tests/core/test_plan_schema.py
# ==============================
# Tests: Plan Proposal Schema
# ==============================
from __future__ import annotations

from core.contracts.plan_schema import EstimatedCost, PlanApproval, PlanProposal, PlanStep


def test_plan_proposal_schema_validates() -> None:
    plan = PlanProposal(
        schema_version="1.0",
        summary="Proposed execution plan",
        steps=[
            PlanStep(step_id="s1", description="Read input", step_type="tool", tool="data_reader"),
            PlanStep(step_id="s2", description="Summarize", step_type="agent", agent="llm_reasoner"),
        ],
        required_tools=["data_reader"],
        approvals=[PlanApproval(step_id="s2", reason="Review narrative")],
        estimated_cost=EstimatedCost(currency="USD", amount=0.12, tokens=1200),
    )
    payload = plan.model_dump(mode="json")
    assert payload["summary"] == "Proposed execution plan"
    assert payload["estimated_cost"]["currency"] == "USD"

# tests/core/test_product_loader.py
# ==============================
# Tests: Product Loader
# ==============================
from __future__ import annotations

from pathlib import Path
from typing import List

import textwrap

from core.config.schema import Settings
from core.utils.product_loader import (
    ProductCatalog,
    ProductLoadError,
    discover_products,
    register_enabled_products,
)


def _make_settings(repo_root: Path, *, enabled: List[str] | None = None, auto_enable: bool = True) -> Settings:
    data = {
        "app": {"paths": {"repo_root": str(repo_root)}},
        "products": {
            "products_dir": "products",
            "enabled": enabled or [],
            "auto_enable": auto_enable,
        },
    }
    return Settings.model_validate(data)


def _write_product(root: Path, name: str, *, with_registry: bool = True) -> None:
    prod_dir = root / "products" / name
    (prod_dir / "flows").mkdir(parents=True, exist_ok=True)
    (prod_dir / "config").mkdir(parents=True, exist_ok=True)
    (prod_dir / "__init__.py").write_text("", encoding="utf-8")

    (prod_dir / "flows" / "flow_one.yaml").write_text("id: flow_one", encoding="utf-8")

    manifest = textwrap.dedent(
        f"""
        name: "{name}"
        display_name: "{name.title()}"
        description: "Test {name}"
        version: "0.1.0"
        default_flow: "flow_one"
        exposed_api:
          enabled: true
        ui_enabled: true
        ui:
          enabled: true
        """
    ).strip()
    (prod_dir / "manifest.yaml").write_text(manifest, encoding="utf-8")

    config = textwrap.dedent(
        f"""
        name: "{name}"
        defaults:
          autonomy_level: "semi_auto"
        """
    ).strip()
    (prod_dir / "config" / "product.yaml").write_text(config, encoding="utf-8")

    if with_registry:
        registry = textwrap.dedent(
            """
            from core.utils.product_loader import ProductRegistries

            def register(registries: ProductRegistries) -> None:
                registries.agent_registry.register("test_agent", lambda: None)
                registries.tool_registry.register("test_tool", lambda: None)
            """
        ).strip()
        (prod_dir / "registry.py").write_text(registry, encoding="utf-8")


class DummyRegistry:
    def __init__(self) -> None:
        self.names: List[str] = []

    def register(self, name: str, factory) -> None:  # pragma: no cover - simple helper
        self.names.append(name)


def test_discovery_and_registration(tmp_path: Path) -> None:
    _write_product(tmp_path, "alpha")
    settings = _make_settings(tmp_path)

    catalog = discover_products(settings, repo_root=tmp_path)
    assert "alpha" in catalog.products
    assert catalog.flows["alpha"] == ["flow_one"]

    agent_reg = DummyRegistry()
    tool_reg = DummyRegistry()
    errors = register_enabled_products(
        catalog,
        settings=settings,
        agent_registry=agent_reg,
        tool_registry=tool_reg,
    )
    assert not errors
    assert agent_reg.names == ["test_agent"]
    assert tool_reg.names == ["test_tool"]


def test_missing_registry_records_error(tmp_path: Path) -> None:
    _write_product(tmp_path, "bravo", with_registry=False)
    settings = _make_settings(tmp_path)

    catalog = discover_products(settings, repo_root=tmp_path)
    assert "bravo" not in catalog.products
    assert any("registry.py" in err.path for err in catalog.errors)


def test_enabled_filtering(tmp_path: Path) -> None:
    _write_product(tmp_path, "alpha")
    _write_product(tmp_path, "beta")
    settings = _make_settings(tmp_path, enabled=["beta"], auto_enable=False)

    catalog = discover_products(settings, repo_root=tmp_path)
    assert catalog.products["alpha"].enabled is False
    assert catalog.products["beta"].enabled is True

# tests/core/test_tools_core.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Callable, List, Tuple

from core.config.schema import Settings
from core.contracts.flow_schema import FlowDef, StepDef, StepType
from core.contracts.run_schema import RunRecord, RunStatus
from core.governance.hooks import GovernanceHooks
from core.governance.security import SecurityRedactor
from core.orchestrator.context import RunContext, StepContext
from core.tools.executor import ToolExecutor
from core.tools.registry import ToolRegistry

from products.hello_world.tools.echo_tool import EchoTool


@dataclass
class RecordingTool(EchoTool):
    calls: int = 0

    def run(self, params, ctx):
        self.calls += 1
        return super().run(params, ctx)


def _build_step_ctx(*, product: str = "hello_world", events: List[Tuple[str, dict]] | None = None) -> StepContext:
    run_record = RunRecord(run_id="run-tools", product=product, flow_id="hello", status=RunStatus.RUNNING)
    flow = FlowDef(id="hello", steps=[StepDef(id="s1", type=StepType.TOOL, tool="echo_tool")])
    trace: Callable[[str, dict], None] | None = None
    if events is not None:
        trace = lambda event_type, payload: events.append((event_type, payload))
    run_ctx = RunContext(run_id=run_record.run_id, product=run_record.product, flow=flow.id, trace=trace)
    return run_ctx.new_step(step_def=flow.steps[0])


def _executor(settings: Settings, registry: ToolRegistry) -> ToolExecutor:
    return ToolExecutor(registry=registry, hooks=GovernanceHooks(settings=settings), redactor=SecurityRedactor())


def test_tool_registry_registers_and_resolves() -> None:
    ToolRegistry.clear()
    registry = ToolRegistry()
    registry.register(name="echo_tool", factory=lambda: RecordingTool())
    resolved = registry.resolve("echo_tool")
    assert isinstance(resolved, EchoTool)


def test_tool_executor_runs_tool_and_redacts_traces() -> None:
    ToolRegistry.clear()
    settings = Settings()
    registry = ToolRegistry()
    tools = []

    def _factory() -> RecordingTool:
        tool = RecordingTool()
        tools.append(tool)
        return tool

    registry.register(name="echo_tool", factory=_factory)
    events: List[Tuple[str, dict]] = []
    ctx = _build_step_ctx(events=events)

    executor = _executor(settings, registry)
    result = executor.execute(tool_name="echo_tool", params={"message": "secret sk-abc"}, ctx=ctx)

    assert result.ok is True
    assert tools and tools[0].calls == 1
    assert result.data and "secret" in result.data["echo"]
    payloads = [payload for event, payload in events if event == "tool.executed"]
    assert payloads
    serialized = str(payloads[0])
    assert "sk-" not in serialized  # redacted
    assert "step_id" in payloads[0]


def test_tool_executor_blocks_denied_tool_without_running_code() -> None:
    ToolRegistry.clear()
    settings = Settings()
    settings.policies.blocked_tools = ["echo_tool"]
    registry = ToolRegistry()
    tools = []

    def _factory() -> RecordingTool:
        tool = RecordingTool()
        tools.append(tool)
        return tool

    registry.register(name="echo_tool", factory=_factory)
    events: List[Tuple[str, dict]] = []
    ctx = _build_step_ctx(events=events)

    executor = _executor(settings, registry)
    result = executor.execute(tool_name="echo_tool", params={"message": "hi"}, ctx=ctx)

    assert result.ok is False
    assert tools and tools[0].calls == 0
    assert any(event == "governance.decision" for event, _ in events)

# tests/core/test_trace_contract.py
# ==============================
# Trace Contract Tests
# ==============================
from __future__ import annotations

from typing import List

from core.agents.registry import AgentRegistry
from core.config.loader import load_settings
from core.tools.registry import ToolRegistry
from core.utils.product_loader import discover_products, register_enabled_products


def _register_products() -> None:
    settings = load_settings()
    AgentRegistry.clear()
    ToolRegistry.clear()
    catalog = discover_products(settings)
    register_enabled_products(catalog, settings=settings)


def test_trace_contract(orchestrator, trace_sink: List[dict]) -> None:
    _register_products()
    started = orchestrator.run_flow(product="hello_world", flow="hello_world", payload={"keyword": "trace secret"})
    assert started.ok
    run_id = started.data["run_id"]
    resumed = orchestrator.resume_run(run_id=run_id, approval_payload={"approved": True})
    assert resumed.ok

    kinds = [event["kind"] for event in trace_sink]
    assert "run_started" in kinds
    assert "step_started" in kinds
    assert "step_completed" in kinds
    assert "run_completed" in kinds
    assert kinds.index("step_started") < kinds.index("step_completed")

    for event in trace_sink:
        assert event.get("run_id") == run_id
        assert event.get("product") == "hello_world"
        assert event.get("flow") == "hello_world"
        assert event.get("ts") and isinstance(event["ts"], int)
        assert event.get("event_type")

# tests/core/test_user_input_pause_resume.py
# ==============================
# Tests: User Input Pause/Resume
# ==============================
from __future__ import annotations

from pathlib import Path

import pytest

from core.agents.registry import AgentRegistry
from core.config.schema import PoliciesConfig, Settings
from core.contracts.run_schema import RunStatus, StepStatus
from core.contracts.tool_schema import ToolMeta, ToolResult
from core.governance.hooks import GovernanceHooks
from core.governance.security import SecurityRedactor
from core.memory.in_memory import InMemoryBackend
from core.memory.router import MemoryRouter
from core.memory.tracing import Tracer
from core.orchestrator.engine import OrchestratorEngine
from core.orchestrator.flow_loader import FlowLoader
from core.orchestrator.step_executor import StepExecutor
from core.tools.base import BaseTool
from core.tools.executor import ToolExecutor
from core.tools.registry import ToolRegistry


class _EchoTool(BaseTool):
    name = "echo_tool"

    def run(self, params, ctx):  # type: ignore[no-untyped-def]
        meta = ToolMeta(tool_name=self.name, backend="local")
        return ToolResult.ok(data={"summary": "ok", "details": params}, meta=meta)


def _write_flow(tmp_path: Path) -> Path:
    flows_dir = tmp_path / "products" / "test_product" / "flows"
    flows_dir.mkdir(parents=True, exist_ok=True)
    flow_path = flows_dir / "test_flow.yaml"
    flow_path.write_text(
        "\n".join(
            [
                'id: "test_flow"',
                'version: "1.0.0"',
                "steps:",
                '  - id: "input"',
                '    type: "user_input"',
                "    params:",
                '      schema_version: "1.0"',
                '      form_id: "notes"',
                '      prompt: "Notes"',
                '      input_type: "text"',
                '      mode: "free_text_input"',
                "      schema:",
                '        type: "object"',
                "        properties:",
                "          text:",
                '            type: "string"',
                "      required:",
                '        - "text"',
                '  - id: "echo"',
                '    type: "tool"',
                '    backend: "local"',
                '    tool: "echo_tool"',
                "    params:",
                '      text: "{{artifacts.user_input.notes.values.text}}"',
                "",
            ]
        ),
        encoding="utf-8",
    )
    return flow_path


def _build_engine(tmp_path: Path, *, settings: Settings | None = None) -> OrchestratorEngine:
    flow_path = _write_flow(tmp_path)
    flow_loader = FlowLoader(products_root=flow_path.parents[2])
    memory = MemoryRouter(backend=InMemoryBackend())
    tracer = Tracer(memory=memory, mirror_to_log=False)
    settings = settings or Settings()
    governance = GovernanceHooks(settings=settings)
    tool_executor = ToolExecutor(registry=ToolRegistry, hooks=governance, redactor=SecurityRedactor())
    step_executor = StepExecutor(tool_executor=tool_executor, governance=governance, agent_registry=AgentRegistry)
    return OrchestratorEngine(
        flow_loader=flow_loader,
        step_executor=step_executor,
        memory=memory,
        tracer=tracer,
        governance=governance,
    )


def test_user_input_pause_and_resume(tmp_path: Path) -> None:
    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        ToolRegistry.register("echo_tool", lambda: _EchoTool())
        engine = _build_engine(tmp_path)

        started = engine.run_flow(product="test_product", flow="test_flow", payload={})
        assert started.ok, started.error
        assert started.data["status"] == RunStatus.PENDING_USER_INPUT.value
        run_id = started.data["run_id"]

        bundle = engine.memory.get_run(run_id)
        assert bundle is not None
        assert bundle.run.status == RunStatus.PENDING_USER_INPUT
        step = next(s for s in bundle.steps if s.step_id == "input")
        assert step.status == StepStatus.PENDING_USER_INPUT

        resumed = engine.resume_run(
            run_id=run_id,
            user_input_response={"form_id": "notes", "values": {"text": "hello"}},
        )
        assert resumed.ok, resumed.error
        assert resumed.data["status"] == RunStatus.COMPLETED.value

        bundle = engine.memory.get_run(run_id)
        assert bundle is not None
        assert bundle.run.status == RunStatus.COMPLETED
        step = next(s for s in bundle.steps if s.step_id == "input")
        assert step.status == StepStatus.COMPLETED
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()


def test_user_input_invalid_response_rejected(tmp_path: Path) -> None:
    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        ToolRegistry.register("echo_tool", lambda: _EchoTool())
        engine = _build_engine(tmp_path)

        started = engine.run_flow(product="test_product", flow="test_flow", payload={})
        assert started.ok, started.error
        run_id = started.data["run_id"]

        rejected = engine.resume_run(
            run_id=run_id,
            user_input_response={"form_id": "notes", "values": {"text": ""}},
        )
        assert not rejected.ok
        assert rejected.error is not None
        assert rejected.error.code == "invalid_input"

        bundle = engine.memory.get_run(run_id)
        assert bundle is not None
        assert bundle.run.status == RunStatus.PENDING_USER_INPUT
        step = next(s for s in bundle.steps if s.step_id == "input")
        assert step.status == StepStatus.PENDING_USER_INPUT
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()


def test_user_input_payload_limit_blocked(tmp_path: Path) -> None:
    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        ToolRegistry.register("echo_tool", lambda: _EchoTool())
        settings = Settings(policies=PoliciesConfig(max_payload_bytes=50))
        engine = _build_engine(tmp_path, settings=settings)

        started = engine.run_flow(product="test_product", flow="test_flow", payload={})
        assert started.ok, started.error
        run_id = started.data["run_id"]

        rejected = engine.resume_run(
            run_id=run_id,
            user_input_response={"form_id": "notes", "values": {"text": "x" * 200}},
        )
        assert not rejected.ok
        assert rejected.error is not None
        assert rejected.error.code == "policy_blocked"

        bundle = engine.memory.get_run(run_id)
        assert bundle is not None
        assert bundle.run.status == RunStatus.PENDING_USER_INPUT
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()

# tests/core/test_user_input_step.py
# ==============================
# Tests: User Input Modes
# ==============================
from __future__ import annotations

from core.contracts.user_input_schema import UserInputModes, UserInputRequest
from core.orchestrator.engine import _validate_user_input_values


def test_user_input_free_text_requires_text() -> None:
    request = UserInputRequest(
        schema_version="1.0",
        form_id="notes",
        prompt="Notes",
        input_type="text",
        mode=UserInputModes.FREE_TEXT_INPUT,
    )
    errors = _validate_user_input_values(request, {"text": ""})
    assert "missing_or_empty:text" in errors

    ok_errors = _validate_user_input_values(request, {"text": "hello"})
    assert ok_errors == []


def test_user_input_choice_input_validates_required() -> None:
    request = UserInputRequest(
        schema_version="1.0",
        form_id="choice",
        prompt="Choice",
        input_type="select",
        mode=UserInputModes.CHOICE_INPUT,
        schema={
            "type": "object",
            "properties": {
                "chart_type": {"type": "string", "enum": ["bar", "line"]},
            },
        },
        required=["chart_type"],
    )
    errors = _validate_user_input_values(request, {})
    assert "missing_required:chart_type" in errors

# tests/generate_component_txt.py
#!/usr/bin/env python3
from __future__ import annotations

import os
from datetime import datetime, timezone
from pathlib import Path
from typing import Iterable, List


REPO_ROOT = Path(__file__).resolve().parents[1]
OUTPUT_DIR = REPO_ROOT / "docs" / "components"
EXTENSIONS = {".py", ".yaml", ".yml"}
EXCLUDE_DIRS: set[str] = set()
EXCLUDE_FILES: set[str] = {".DS_Store"}


def _iter_components(root: Path) -> Iterable[Path]:
    for path in root.rglob("*"):
        if not path.is_file():
            continue
        if path.name in EXCLUDE_FILES:
            continue
        if path.parent == root:
            yield path
            continue
        if path.suffix.lower() in EXTENSIONS:
            yield path


def _top_level_dir(path: Path) -> str:
    rel = path.relative_to(REPO_ROOT)
    parts = rel.parts
    return parts[0] if len(parts) > 1 else "root"


def _write_bundle(name: str, files: List[Path]) -> None:
    if not files:
        return
    output_path = OUTPUT_DIR / f"{name}.txt"
    timestamp = _timestamp()
    lines: List[str] = []
    lines.append(f"# captured_at: {timestamp}")
    lines.append("")
    for file_path in sorted(files):
        rel = file_path.relative_to(REPO_ROOT)
        lines.append(f"# {rel}")
        try:
            content = file_path.read_text(encoding="utf-8")
        except Exception:
            content = file_path.read_text(encoding="utf-8", errors="replace")
        if "\x00" in content:
            continue
        if _is_secrets_path(file_path):
            content = _redact_secrets(content)
        lines.append(content.rstrip())
        lines.append("")
    output_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")


def _timestamp() -> str:
    return datetime.now(tz=timezone.utc).isoformat()


def _is_secrets_path(path: Path) -> bool:
    return "secrets" in path.parts


def _redact_secrets(content: str) -> str:
    redacted_lines: List[str] = []
    for line in content.splitlines():
        stripped = line.strip()
        if not stripped or stripped.startswith("#") or ":" not in line:
            redacted_lines.append(line)
            continue
        prefix, _sep, _rest = line.partition(":")
        redacted_lines.append(f"{prefix}: ***REDACTED***")
    return "\n".join(redacted_lines)


def main() -> None:
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    bundles: dict[str, List[Path]] = {}
    for component in _iter_components(REPO_ROOT):
        top = _top_level_dir(component)
        if not top or top.startswith(".") or top in EXCLUDE_DIRS:
            continue
        bundles.setdefault(top, []).append(component)
    for name, files in sorted(bundles.items()):
        _write_bundle(name, files)
    print(f"Wrote {len(bundles)} component bundle(s) to {OUTPUT_DIR}")


if __name__ == "__main__":
    main()

# tests/integration/conftest.py
# ==============================
# Integration fixtures
from __future__ import annotations

from pathlib import Path

import pytest


@pytest.fixture
def hello_world_test_env(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> Path:
    """
    Shared env fixture for integration tests that rely on sqlite-backed memory.

    This fixture ensures we use deterministic sqlite paths (for runs, approvals, and
    ingestion) without duplicating environment overrides in every test.
    """
    repo_root = Path(__file__).resolve().parents[2]
    storage_dir = tmp_path / "storage"
    sqlite_path = tmp_path / "integration.sqlite"

    monkeypatch.setenv("MASTER__APP__PATHS__REPO_ROOT", repo_root.as_posix())
    monkeypatch.setenv("MASTER__APP__PATHS__STORAGE_DIR", storage_dir.as_posix())
    monkeypatch.setenv("MASTER__SECRETS__MEMORY_DB_PATH", sqlite_path.as_posix())

    return sqlite_path

# tests/integration/test_api_responsiveness.py
# ==============================
# Integration: API Responsiveness
# ==============================
from __future__ import annotations

from pathlib import Path
import threading
import time

import pytest
from fastapi.testclient import TestClient

from core.contracts.run_schema import RunOperationResult
from core.agents.registry import AgentRegistry
from core.tools.registry import ToolRegistry
from gateway.api.http_app import create_app
import gateway.api.deps as deps


def _reset_deps() -> None:
    for dep in (
        deps.get_engine,
        deps.get_settings,
        deps.get_memory_router,
        deps.get_tracer,
        deps.get_product_catalog,
    ):
        cache_clear = getattr(dep, "cache_clear", None)
        if callable(cache_clear):
            cache_clear()


class _SlowEngine:
    def __init__(self, *, started: threading.Event, finished: threading.Event, delay_seconds: float) -> None:
        self._started = started
        self._finished = finished
        self._delay = delay_seconds

    def run_flow(self, *, product: str, flow: str, payload: dict, requested_by: str | None = None) -> RunOperationResult:
        self._started.set()
        time.sleep(self._delay)
        self._finished.set()
        return RunOperationResult.success({"run_id": "slow_run", "status": "PENDING_HUMAN"})


@pytest.mark.integration
def test_health_responsive_during_slow_run(tmp_path, monkeypatch) -> None:
    repo_root = Path(__file__).resolve().parents[2]
    sqlite_path = tmp_path / "api.sqlite"
    storage_dir = tmp_path / "storage"
    monkeypatch.setenv("MASTER__APP__PATHS__REPO_ROOT", repo_root.as_posix())
    monkeypatch.setenv("MASTER__APP__PATHS__STORAGE_DIR", storage_dir.as_posix())
    monkeypatch.setenv("MASTER__SECRETS__MEMORY_DB_PATH", sqlite_path.as_posix())

    started = threading.Event()
    finished = threading.Event()
    slow_engine = _SlowEngine(started=started, finished=finished, delay_seconds=0.6)

    _reset_deps()
    AgentRegistry.clear()
    ToolRegistry.clear()
    app = create_app()
    app.dependency_overrides[deps.get_engine] = lambda: slow_engine
    client = TestClient(app)

    def _run_request() -> None:
        resp = client.post("/api/run/hello_world/hello_world", json={"payload": {"keyword": "slow"}})
        body = resp.json()
        assert body["ok"] is True

    run_thread = threading.Thread(target=_run_request)
    run_thread.start()

    assert started.wait(timeout=2.0)

    start = time.monotonic()
    health = client.get("/health")
    elapsed = time.monotonic() - start

    assert health.status_code == 200
    assert elapsed < 0.3
    assert not finished.is_set()

    run_thread.join(timeout=2.0)
    assert finished.is_set()
    client.close()

# tests/integration/test_api_runs.py
# ==============================
# Integration: Gateway API Runs
# ==============================
from __future__ import annotations

from pathlib import Path
import json
from typing import Any, Dict, Optional

import pytest
from fastapi.testclient import TestClient

from core.agents.registry import AgentRegistry
from core.tools.registry import ToolRegistry
from gateway.api.http_app import create_app
import gateway.api.deps as deps


def _reset_deps() -> None:
    for dep in (
        deps.get_engine,
        deps.get_settings,
        deps.get_memory_router,
        deps.get_tracer,
        deps.get_product_catalog,
    ):
        cache_clear = getattr(dep, "cache_clear", None)
        if callable(cache_clear):
            cache_clear()


@pytest.fixture()
def api_client(tmp_path, monkeypatch):
    repo_root = Path(__file__).resolve().parents[2]
    sqlite_path = tmp_path / "api.sqlite"
    storage_dir = tmp_path / "storage"
    monkeypatch.setenv("MASTER__APP__PATHS__REPO_ROOT", repo_root.as_posix())
    monkeypatch.setenv("MASTER__APP__PATHS__STORAGE_DIR", storage_dir.as_posix())
    monkeypatch.setenv("MASTER__SECRETS__MEMORY_DB_PATH", sqlite_path.as_posix())
    AgentRegistry.clear()
    ToolRegistry.clear()
    _reset_deps()
    client = TestClient(create_app())
    yield client
    client.close()
    AgentRegistry.clear()
    ToolRegistry.clear()
    _reset_deps()


def _start_hello_world_run(api_client: TestClient, payload: Optional[Dict[str, Any]] = None) -> str:
    req_payload = payload or {"keyword": "API"}
    started = api_client.post("/api/run/hello_world/hello_world", json={"payload": req_payload}).json()
    assert started["ok"] is True
    assert started["data"]["status"] == "PENDING_HUMAN"
    return started["data"]["run_id"]


@pytest.mark.integration
def test_gateway_api_run_resume_flow(api_client: TestClient) -> None:
    products = api_client.get("/api/products").json()
    assert products["ok"] is True
    hello_world = next((p for p in products["data"]["products"] if p["name"] == "hello_world"), None)
    assert hello_world is not None
    assert "hello_world" in hello_world["flows"]

    flows = api_client.get("/api/products/hello_world/flows").json()
    assert flows["ok"] is True
    assert "hello_world" in flows["data"]["flows"]

    run_id = _start_hello_world_run(api_client)

    pending = api_client.get(f"/api/run/{run_id}").json()
    assert pending["ok"] is True
    assert pending["data"]["run"]["status"] == "PENDING_HUMAN"

    resumed = api_client.post(
        f"/api/resume_run/{run_id}",
        json={"decision": "APPROVED", "approval_payload": {"approved": True, "notes": "ok"}},
    ).json()
    assert resumed["ok"] is True
    assert resumed["data"]["status"] == "COMPLETED"

    final = api_client.get(f"/api/run/{run_id}").json()
    assert final["ok"] is True
    assert final["data"]["run"]["status"] == "COMPLETED"


@pytest.mark.integration
def test_gateway_api_resume_rejection_marks_failed(api_client: TestClient) -> None:
    run_id = _start_hello_world_run(api_client)

    resumed = api_client.post(
        f"/api/resume_run/{run_id}",
        json={"decision": "APPROVED", "approval_payload": {"approved": False, "notes": "reject"}},
    ).json()
    assert resumed["ok"] is True
    assert resumed["data"]["status"] == "FAILED"

    final = api_client.get(f"/api/run/{run_id}").json()
    assert final["ok"] is True
    assert final["data"]["run"]["status"] == "FAILED"


@pytest.mark.integration
def test_gateway_api_missing_approval_field_is_rejected(api_client: TestClient) -> None:
    run_id = _start_hello_world_run(api_client)
    resp = api_client.post(
        f"/api/resume_run/{run_id}",
        json={"decision": "APPROVED", "approval_payload": {"notes": "missing approved flag"}},
    )
    assert resp.status_code == 400
    body = resp.json()
    assert body["detail"]["error"]["code"] == "missing_approval_field"


@pytest.mark.integration
def test_gateway_api_trace_cleanliness(api_client: TestClient) -> None:
    run_id = _start_hello_world_run(api_client, payload={"keyword": "safe", "api_key": "sk-secret"})
    resumed = api_client.post(
        f"/api/resume_run/{run_id}",
        json={"decision": "APPROVED", "approval_payload": {"approved": True}},
    ).json()
    assert resumed["ok"] is True
    final = api_client.get(f"/api/run/{run_id}").json()
    serialized_steps = json.dumps(final["data"].get("steps", []))
    assert "api_key" not in serialized_steps

# tests/integration/test_cli_runs.py
# ==============================
# Integration: CLI Runs
# ==============================
from __future__ import annotations

import json
from pathlib import Path
from typing import List, Tuple

import pytest

from core.agents.registry import AgentRegistry
from core.tools.registry import ToolRegistry
from gateway.cli import main as cli_main


def _run_cli(args: List[str], capsys) -> Tuple[int, dict]:
    code = cli_main.main(args)
    captured = capsys.readouterr()
    output = captured.out.strip()
    data = json.loads(output) if output else {}
    return code, data


@pytest.mark.integration
def test_cli_run_resume_flow(tmp_path, monkeypatch, capsys) -> None:
    repo_root = Path(__file__).resolve().parents[2]
    storage_dir = tmp_path / "storage"
    sqlite_path = tmp_path / "cli.sqlite"
    monkeypatch.setenv("MASTER__APP__PATHS__REPO_ROOT", repo_root.as_posix())
    monkeypatch.setenv("MASTER__APP__PATHS__STORAGE_DIR", storage_dir.as_posix())
    monkeypatch.setenv("MASTER__SECRETS__MEMORY_DB_PATH", sqlite_path.as_posix())

    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        code, products = _run_cli(["list-products"], capsys)
        assert code == 0
        assert "hello_world" in products["products"]

        code, flows = _run_cli(["list-flows", "--product", "hello_world"], capsys)
        assert code == 0
        assert "hello_world" in flows["flows"]

        code, started = _run_cli(
            [
                "run",
                "--product",
                "hello_world",
                "--flow",
                "hello_world",
                "--payload",
                '{"keyword":"CLI"}',
            ],
            capsys,
        )
        assert code == 0
        assert started["ok"] is True
        run_id = started["data"]["run_id"]
        assert started["data"]["status"] == "PENDING_HUMAN"

        code, pending = _run_cli(["status", "--run-id", run_id], capsys)
        assert code == 0
        assert pending["ok"] is True
        assert pending["data"]["run"]["status"] == "PENDING_HUMAN"

        code, approvals = _run_cli(["approvals"], capsys)
        assert code == 0
        assert approvals["approvals"], "Expected at least one pending approval"

        code, resumed = _run_cli(
            [
                "resume",
                "--run-id",
                run_id,
                "--approve",
                "--payload",
                '{"approved": true}',
            ],
            capsys,
        )
        assert code == 0
        assert resumed["ok"] is True

        code, final = _run_cli(["status", "--run-id", run_id], capsys)
        assert code == 0
        assert final["data"]["run"]["status"] == "COMPLETED"
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()


@pytest.mark.integration
def test_cli_resume_rejection_marks_failed(tmp_path, monkeypatch, capsys) -> None:
    repo_root = Path(__file__).resolve().parents[2]
    storage_dir = tmp_path / "storage"
    sqlite_path = tmp_path / "cli.sqlite"
    monkeypatch.setenv("MASTER__APP__PATHS__REPO_ROOT", repo_root.as_posix())
    monkeypatch.setenv("MASTER__APP__PATHS__STORAGE_DIR", storage_dir.as_posix())
    monkeypatch.setenv("MASTER__SECRETS__MEMORY_DB_PATH", sqlite_path.as_posix())

    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        _, _ = _run_cli(["list-products"], capsys)
        _, _ = _run_cli(["list-flows", "--product", "hello_world"], capsys)

        _, started = _run_cli(
            [
                "run",
                "--product",
                "hello_world",
                "--flow",
                "hello_world",
                "--payload",
                '{"keyword":"CLI"}',
            ],
            capsys,
        )
        run_id = started["data"]["run_id"]

        _, rejection = _run_cli(
            [
                "resume",
                "--run-id",
                run_id,
                "--reject",
                "--payload",
                '{"approved": false}',
            ],
            capsys,
        )
        assert rejection["ok"] is True
        assert rejection["data"]["status"] == "FAILED"

        _, final = _run_cli(["status", "--run-id", run_id], capsys)
        assert final["data"]["run"]["status"] == "FAILED"
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()


@pytest.mark.integration
def test_cli_resume_missing_payload_fails(tmp_path, monkeypatch, capsys) -> None:
    repo_root = Path(__file__).resolve().parents[2]
    storage_dir = tmp_path / "storage"
    sqlite_path = tmp_path / "cli.sqlite"
    monkeypatch.setenv("MASTER__APP__PATHS__REPO_ROOT", repo_root.as_posix())
    monkeypatch.setenv("MASTER__APP__PATHS__STORAGE_DIR", storage_dir.as_posix())
    monkeypatch.setenv("MASTER__SECRETS__MEMORY_DB_PATH", sqlite_path.as_posix())

    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        _, _ = _run_cli(["list-products"], capsys)
        _, _ = _run_cli(["list-flows", "--product", "hello_world"], capsys)

        _, started = _run_cli(
            [
                "run",
                "--product",
                "hello_world",
                "--flow",
                "hello_world",
                "--payload",
                '{"keyword":"CLI"}',
            ],
            capsys,
        )
        run_id = started["data"]["run_id"]

        code, failure = _run_cli(
            [
                "resume",
                "--run-id",
                run_id,
                "--approve",
            ],
            capsys,
        )
        assert code != 0
        assert failure["ok"] is False
        assert failure["error"]["code"] == "missing_approval_field"
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()

# tests/integration/test_concurrency_isolation.py
# ==============================
# Concurrency Isolation Tests
# ==============================
from __future__ import annotations

from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List

from core.agents.registry import AgentRegistry
from core.config.loader import load_settings
from core.memory.router import MemoryRouter
from core.tools.registry import ToolRegistry
from core.utils.product_loader import discover_products, register_enabled_products


def _register_products():
    settings = load_settings()
    AgentRegistry.clear()
    ToolRegistry.clear()
    catalog = discover_products(settings)
    register_enabled_products(catalog, settings=settings)
    return settings


def _run_and_finish(orchestrator, trace_sink) -> str:
    res = orchestrator.run_flow(product="hello_world", flow="hello_world", payload={"keyword": "hello"})
    assert res.ok
    run_id = res.data["run_id"]
    orchestrator.resume_run(run_id=run_id, approval_payload={"approved": True})
    return run_id


def test_concurrent_runs_isolated(orchestrator, trace_sink) -> None:
    _register_products()
    executor = ThreadPoolExecutor(max_workers=3)
    futures = [executor.submit(_run_and_finish, orchestrator, trace_sink) for _ in range(3)]
    run_ids = [f.result() for f in futures]
    executor.shutdown()

    memory = orchestrator.memory  # type: ignore[assignment]
    for run_id in run_ids:
        bundle = memory.get_run(run_id)
        assert bundle is not None
        assert bundle.run.run_id == run_id
        assert bundle.run.status == "COMPLETED"

    trace_runs = {event["run_id"] for event in trace_sink}
    assert set(run_ids) == trace_runs
    assert all(run_id in trace_runs for run_id in run_ids)

    # Shared fixture sanity: each run_id has its own entries, no cross-run leaks
    run_steps = {run_id: [e for e in trace_sink if e["run_id"] == run_id] for run_id in run_ids}
    for run_id, events in run_steps.items():
        assert all(event["run_id"] == run_id for event in events)
        assert events, "Expected trace events per run"

# tests/integration/test_parallel_failure_isolation.py
# ==============================
# Parallel Failure Isolation
# ==============================
from __future__ import annotations

from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List

from core.agents.registry import AgentRegistry
from core.config.loader import load_settings
from core.contracts.tool_schema import ToolError, ToolErrorCode, ToolMeta, ToolResult
from core.memory.router import MemoryRouter
from core.tools.base import BaseTool
from core.tools.registry import ToolRegistry, ToolRegistration
from core.utils.product_loader import discover_products, register_enabled_products


def _register_products():
    settings = load_settings()
    AgentRegistry.clear()
    ToolRegistry.clear()
    catalog = discover_products(settings)
    register_enabled_products(catalog, settings=settings)
    return settings


class PayloadDrivenTool(BaseTool):
    name: str = "echo_tool"

    def run(self, params, ctx):
        fail = bool(ctx.run.payload.get("fail_run"))
        meta = ToolMeta(tool_name=self.name, backend="parallel")
        if fail:
            err = ToolError(
                code=ToolErrorCode.BACKEND_ERROR,
                message="forced failure",
                details={"run_id": ctx.run_id},
            )
            return ToolResult(ok=False, data=None, error=err, meta=meta)
        return ToolResult(ok=True, data={"echo": params.get("message", "")}, error=None, meta=meta)


def _override_echo(tool_cls) -> ToolRegistration | None:
    original = ToolRegistry._tools.get("echo_tool")
    ToolRegistry.register("echo_tool", tool_cls, overwrite=True)
    return original


def _restore_echo(registration: ToolRegistration | None) -> None:
    if registration:
        ToolRegistry._tools["echo_tool"] = registration
    else:
        ToolRegistry._tools.pop("echo_tool", None)


def test_parallel_failure_isolation(orchestrator, trace_sink: List[Dict[str, str]]) -> None:
    _register_products()
    original = _override_echo(lambda: PayloadDrivenTool())
    try:
        def run_task(marker: str, fail: bool):
            payload = {"keyword": f"run-{marker}", "fail_run": fail}
            res = orchestrator.run_flow(product="hello_world", flow="hello_world", payload=payload)
            return res.data["run_id"], fail

        results = []
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [
                executor.submit(run_task, "a", False),
                executor.submit(run_task, "b", False),
                executor.submit(run_task, "c", True),
            ]
            for future in futures:
                results.append(future.result())

        memory = orchestrator.memory  # type: ignore[assignment]
        failed_runs = []
        success_runs = []
        for run_id, is_failed in results:
            bundle = memory.get_run(run_id)
            assert bundle
            if is_failed:
                failed_runs.append(run_id)
                assert bundle.run.status != "COMPLETED"
            else:
                success_runs.append(run_id)
                orchestrator.resume_run(run_id=run_id, approval_payload={"approved": True})
                bundle = memory.get_run(run_id)
                assert bundle.run.status == "COMPLETED"

        assert len(failed_runs) == 1
        assert len(success_runs) == 2

        failure_events = [event for event in trace_sink if event["run_id"] in failed_runs]
        assert failure_events, f"No trace events for failed runs: {failed_runs}"
        assert any(event["kind"] == "tool_call_attempt_failed" for event in failure_events), f"Events: {[e['kind'] for e in failure_events]}"
        assert all(
            event["run_id"] in failed_runs
            for event in failure_events
            if event["kind"] == "tool_call_attempt_failed"
        )

        success_events = [event for event in trace_sink if event["run_id"] in success_runs]
        assert all(event["kind"] != "step_failed" for event in success_events)
        assert success_events
    finally:
        _restore_echo(original)

# tests/integration/test_product_discovery.py
# ==============================
# Integration: Product Discovery
# ==============================
from __future__ import annotations

from core.config.loader import load_settings
from core.utils.product_loader import discover_products


def test_catalog_contains_hello_world() -> None:
    settings = load_settings()
    catalog = discover_products(settings)
    assert "hello_world" in catalog.products
    assert "hello_world" in catalog.flows.get("hello_world", [])

# tests/integration/test_resilience_retries_timeouts.py
# ==============================
# Resilience: Retries & Timeouts
# ==============================
from __future__ import annotations

from typing import Dict, List, Type

from core.agents.registry import AgentRegistry
from core.config.loader import load_settings
from core.contracts.run_schema import RunStatus
from core.contracts.tool_schema import ToolError, ToolErrorCode, ToolMeta, ToolResult
from core.tools.base import BaseTool
from core.tools.registry import ToolRegistry, ToolRegistration
from core.utils.product_loader import discover_products, register_enabled_products


def _register_products():
    settings = load_settings()
    AgentRegistry.clear()
    ToolRegistry.clear()
    catalog = discover_products(settings)
    register_enabled_products(catalog, settings=settings)
    return settings


def _override_echo_tool(factory_cls: Type[BaseTool]) -> ToolRegistration | None:
    reg = ToolRegistry._tools.get("echo_tool")
    ToolRegistry.register("echo_tool", lambda: factory_cls(), overwrite=True)
    return reg


def _restore_echo_tool(registration: ToolRegistration | None) -> None:
    if registration:
        ToolRegistry._tools["echo_tool"] = registration
    else:
        ToolRegistry._tools.pop("echo_tool", None)


class BackendBehaviorTool(BaseTool):
    """Tool wrapper that delegates to a backend function defined by tests."""

    name: str = "echo_tool"

    def __init__(self, *, behavior: str, state: Dict[str, int]) -> None:
        self.behavior = behavior
        self.state = state
        super().__init__()

    def run(self, params, ctx):
        key = (ctx.run_id, ctx.step_id)
        self.state[key] = self.state.get(key, 0) + 1
        meta = ToolMeta(tool_name=self.name, backend="test")
        if self.behavior == "fail_once_then_success" and self.state[key] == 1:
            err = ToolError(
                code=ToolErrorCode.TEMPORARY,
                message="simulated failure",
                details={"phase": "first"},
            )
            return ToolResult(ok=False, data=None, error=err, meta=meta)
        if self.behavior == "always_timeout":
            err = ToolError(
                code=ToolErrorCode.TIMEOUT,
                message="simulated timeout",
                details={"phase": "timeout"},
            )
            return ToolResult(ok=False, data=None, error=err, meta=meta)
        return ToolResult(ok=True, data={"result": "ok"}, error=None, meta=meta)


def test_retry_success(orchestrator, trace_sink: List[Dict[str, Any]]) -> None:
    _register_products()
    state: Dict = {}
    original = _override_echo_tool(lambda: BackendBehaviorTool(behavior="fail_once_then_success", state=state))
    try:
        result = orchestrator.run_flow(product="hello_world", flow="hello_world", payload={"keyword": "retry"})
        assert result.ok, result.error
        assert state, "Tool attempts recorded"
        tool_event_kinds = [e["kind"] for e in trace_sink if e["kind"].startswith("tool_call")]
        assert tool_event_kinds == [
            "tool_call_attempt_started",
            "tool_call_attempt_failed",
            "tool_call_retry_scheduled",
            "tool_call_attempt_started",
            "tool_call_succeeded",
        ]
        resumed = orchestrator.resume_run(run_id=result.data["run_id"], approval_payload={"approved": True})
        assert resumed.ok
        run = orchestrator.get_run(run_id=result.data["run_id"])
        assert run.ok
        assert run.data["run"]["status"] == RunStatus.COMPLETED.value
    finally:
        _restore_echo_tool(original)


def test_timeout_exhaustion(orchestrator, trace_sink: List[Dict[str, Any]]) -> None:
    _register_products()
    state: Dict = {}
    original = _override_echo_tool(lambda: BackendBehaviorTool(behavior="always_timeout", state=state))
    try:
        result = orchestrator.run_flow(product="hello_world", flow="hello_world", payload={"keyword": "timeout"})
        assert result.ok
        assert state, "Timeout attempts recorded"
        tool_event_kinds = [e["kind"] for e in trace_sink if e["kind"].startswith("tool_call")]
        assert tool_event_kinds == [
            "tool_call_attempt_started",
            "tool_call_attempt_failed",
            "tool_call_retry_scheduled",
            "tool_call_attempt_started",
            "tool_call_attempt_failed",
        ]
        run = orchestrator.get_run(run_id=result.data["run_id"])
        assert run.ok
        assert run.data["run"]["status"] == RunStatus.FAILED.value
    finally:
        _restore_echo_tool(original)

# tests/integration/test_sample_flows.py
# ==============================
# Integration Tests: Golden Path (Core + Hello World Product)
# ==============================
from __future__ import annotations

from pathlib import Path

import pytest

from core.config.loader import load_settings
from core.utils.product_loader import discover_products, register_enabled_products
from core.orchestrator.engine import OrchestratorEngine
from core.agents.registry import AgentRegistry
from core.tools.registry import ToolRegistry


@pytest.mark.integration
def test_sample_flow_hello_world(tmp_path: Path, hello_world_test_env: Path) -> None:
    """
    Runs:
      echo -> HITL -> summary

    The hello_world_test_env fixture handles sqlite/secrets overrides so other integration suites can reuse the same storage location.
    """
    repo_root = Path(__file__).resolve().parents[2]
    configs_dir = repo_root / "configs"
    settings = load_settings(configs_dir=str(configs_dir))
    AgentRegistry.clear()
    ToolRegistry.clear()
    try:
        catalog = discover_products(settings, repo_root=repo_root)
        register_enabled_products(catalog, settings=settings)

        engine = OrchestratorEngine.from_settings(settings)

        started = engine.run_flow(product="hello_world", flow="hello_world", payload={"keyword": "hello"})
        assert started.ok, started.error
        run_id = started.data["run_id"]  # type: ignore[index]

        # Approve
        resumed = engine.resume_run(run_id=run_id, approval_payload={"approved": True, "notes": "ok"})
        assert resumed.ok, resumed.error

        final = engine.get_run(run_id=run_id)
        assert final.ok, final.error
        assert final.data and final.data["run"]["status"] in ("COMPLETED", "completed")
    finally:
        AgentRegistry.clear()
        ToolRegistry.clear()

# tests/integration/test_session_isolation.py
# ==============================
# Integration: Session Isolation
# ==============================
from __future__ import annotations

from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Dict

import pytest

from core.config.loader import load_settings
from core.memory.router import MemoryRouter
from core.memory.sqlite_backend import SQLiteBackend
from core.memory.tracing import Tracer
from core.orchestrator.engine import OrchestratorEngine


@pytest.mark.integration
def test_concurrent_runs_isolated(tmp_path) -> None:
    repo_root = Path(__file__).resolve().parents[2]
    sqlite_path = tmp_path / "api.sqlite"
    storage_dir = tmp_path / "storage"
    env = {
        "MASTER__APP__PATHS__REPO_ROOT": repo_root.as_posix(),
        "MASTER__APP__PATHS__STORAGE_DIR": storage_dir.as_posix(),
        "MASTER__SECRETS__MEMORY_DB_PATH": sqlite_path.as_posix(),
    }
    settings = load_settings(repo_root=str(repo_root), env=env)
    backend = SQLiteBackend(db_path=sqlite_path.as_posix())
    backend.ensure_schema()
    memory = MemoryRouter(backend, repo_root=None)
    tracer = Tracer.from_settings(settings=settings, memory=memory)

    payloads = [{"keyword": "alpha"}, {"keyword": "beta"}]

    def _start_run(payload: Dict[str, str]) -> str:
        engine = OrchestratorEngine.from_settings(settings=settings, memory=memory, tracer=tracer)
        result = engine.run_flow(product="hello_world", flow="hello_world", payload=payload)
        assert result.ok is True
        return result.data["run_id"]

    with ThreadPoolExecutor(max_workers=2) as executor:
        run_ids = list(executor.map(_start_run, payloads))

    assert len(set(run_ids)) == 2

    for payload, run_id in zip(payloads, run_ids):
        bundle = memory.get_run(run_id)
        assert bundle is not None
        assert bundle.run.input == payload

# tests/integration/test_smoke.py
# ==============================
# Integration: Minimal Smoke Test
# ==============================
from __future__ import annotations

from pathlib import Path

from core.config.loader import load_settings
from core.orchestrator.engine import OrchestratorEngine
from core.utils.product_loader import discover_products, register_enabled_products


def test_smoke_engine_init(tmp_path: Path) -> None:
    repo_root = Path(__file__).resolve().parents[2]
    sqlite_path = tmp_path / "smoke.sqlite"
    storage_dir = tmp_path / "storage"

    settings = load_settings(
        repo_root=str(repo_root),
        configs_dir=str(repo_root / "configs"),
        env={
            "MASTER__APP__PATHS__STORAGE_DIR": storage_dir.as_posix(),
            "MASTER__SECRETS__MEMORY_DB_PATH": sqlite_path.as_posix(),
        },
    )
    catalog = discover_products(settings, repo_root=repo_root)
    register_enabled_products(catalog, settings=settings)
    engine = OrchestratorEngine.from_settings(settings)
    assert engine is not None

# tests/integration/test_ui_smoke.py
from typing import Any

# ==============================
# UI Smoke Test
# ==============================
import json
from types import SimpleNamespace


class _FakeResponse:
    def __init__(self, body: dict, ok: bool = True) -> None:
        self._body = body
        self.ok = ok

    def json(self) -> dict:
        return self._body


class _FakeStreamlit:
    def __init__(self) -> None:
        self.calls: list[tuple[str, str]] = []
        self.session_state: dict[str, Any] = {}

    def subheader(self, value: str) -> None:
        self.calls.append(("subheader", value))

    def info(self, message: str) -> None:
        self.calls.append(("info", message))

    def write(self, value: Any) -> None:
        self.calls.append(("write", str(value)))

    def markdown(self, value: str) -> None:
        self.calls.append(("markdown", value))

    def expander(self, label: str, *, expanded: bool = False):
        self.calls.append(("expander_open", label))
        return self

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc, tb):
        self.calls.append(("expander_close", ""))

    def table(self, value: Any) -> None:
        self.calls.append(("table", json.dumps(value)))

    def columns(self, count: int):
        return (self, self)


def test_ui_imports_without_errors(monkeypatch):
    import importlib
    import sys

    evt = SimpleNamespace()
    monkeypatch.setitem(sys.modules, "streamlit", evt)
    module = importlib.import_module("gateway.ui.platform_app")
    assert hasattr(module, "main")


def test_api_client_list_products(monkeypatch):
    import gateway.ui.platform_app as platform_app

    stub_body = {"ok": True, "data": {"products": [{"name": "hello_world", "display_name": "Hello World", "flows": ["hello_world"]}]}}
    monkeypatch.setattr(platform_app.requests, "get", lambda *args, **kwargs: _FakeResponse(stub_body))
    client = platform_app.ApiClient("https://api.example.com")
    resp = client.list_products()
    assert resp.ok
    assert resp.body["data"]["products"][0]["name"] == "hello_world"


def test_product_summary_render(monkeypatch):
    import gateway.ui.platform_app as platform_app

    stub_st = _FakeStreamlit()
    monkeypatch.setattr(platform_app, "st", stub_st)
    products = [{"name": "hello_world", "display_name": "Hello World", "description": "Demo", "flows": ["hello_world"]}]
    platform_app._render_product_summary(products)
    assert any(call[0] == "subheader" for call in stub_st.calls)
    assert any("Hello World" in call[1] for call in stub_st.calls if call[0] == "expander_open")

# tests/integration/test_v1_runtime_does_not_require_knowledge.py
from __future__ import annotations

import importlib
import sys
from pathlib import Path


def _import(module_name: str) -> None:
    importlib.import_module(module_name)


def _knowledge_modules(loaded: set[str]) -> set[str]:
    return {
        name
        for name in loaded
        if name == "core.knowledge" or name.startswith("core.knowledge.")
    }


def test_v1_runtime_imports_do_not_require_knowledge(tmp_path, monkeypatch) -> None:
    repo_root = Path(__file__).resolve().parents[2]
    storage_dir = tmp_path / "storage"

    monkeypatch.setenv("MASTER__APP__PATHS__REPO_ROOT", repo_root.as_posix())
    monkeypatch.setenv("MASTER__APP__PATHS__STORAGE_DIR", storage_dir.as_posix())

    before_modules = set(sys.modules)

    _import("gateway.api.http_app")
    _import("gateway.ui.platform_app")

    new_modules = set(sys.modules) - before_modules
    knowledge_loaded = _knowledge_modules(new_modules)

    assert not knowledge_loaded, f"Unexpected knowledge modules loaded: {sorted(knowledge_loaded)}"
    assert not (storage_dir / "vectors").exists(), "Vector store initialized during import"

# tests/unit/test_agents_no_memory_backend_imports.py
# ==============================
# Tests: Agents must not import memory backends
# ==============================
from __future__ import annotations

import ast
from pathlib import Path
from typing import Iterable, List, Tuple


FORBIDDEN_PREFIXES = (
    "core.memory.sqlite_backend",
    "core.memory.observability_store",
    "core.memory.router",
)


def _iter_python_files(root: Path) -> Iterable[Path]:
    for path in root.rglob("*.py"):
        if path.is_file():
            yield path


def _check_file(path: Path) -> List[Tuple[str, str]]:
    source = path.read_text(encoding="utf-8")
    tree = ast.parse(source, filename=str(path))
    offenders: List[Tuple[str, str]] = []
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                name = alias.name
                if any(name.startswith(prefix) for prefix in FORBIDDEN_PREFIXES):
                    offenders.append((str(path), name))
        elif isinstance(node, ast.ImportFrom):
            module = node.module
            if module and any(module.startswith(prefix) for prefix in FORBIDDEN_PREFIXES):
                offenders.append((str(path), module))
    return offenders


def test_agents_do_not_import_memory_backends() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    agent_roots = [
        repo_root / "core" / "agents",
        repo_root / "products",
    ]
    offenders: List[Tuple[str, str]] = []
    for root in agent_roots:
        if not root.exists():
            continue
        for path in _iter_python_files(root):
            if "agents" not in path.parts:
                continue
            offenders.extend(_check_file(path))
    if offenders:
        details = "\n".join(f"{path}: {module}" for path, module in offenders)
        raise AssertionError(f"Forbidden memory backend imports found in agent code:\n{details}")

# tests/unit/test_architecture_guardrails.py
# ==============================
# Tests: Architecture Guardrails
# ==============================
from __future__ import annotations

import ast
from pathlib import Path
from typing import Iterable, List, Tuple


FORBIDDEN_PREFIXES = (
    "core.models.providers",
    "core.models.router",
    "core.tools.executor",
    "core.memory",
)


def _iter_python_files(root: Path) -> Iterable[Path]:
    for path in root.rglob("*.py"):
        if path.is_file():
            yield path


def _check_file(path: Path) -> List[Tuple[str, str]]:
    source = path.read_text(encoding="utf-8")
    tree = ast.parse(source, filename=str(path))
    offenders: List[Tuple[str, str]] = []
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                name = alias.name
                if any(name.startswith(prefix) for prefix in FORBIDDEN_PREFIXES):
                    offenders.append((str(path), name))
        elif isinstance(node, ast.ImportFrom):
            module = node.module
            if module and any(module.startswith(prefix) for prefix in FORBIDDEN_PREFIXES):
                offenders.append((str(path), module))
    return offenders


def test_products_do_not_import_forbidden_core_modules() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    products_root = repo_root / "products"
    offenders: List[Tuple[str, str]] = []
    for path in _iter_python_files(products_root):
        offenders.extend(_check_file(path))
    if offenders:
        details = "\n".join(f"{path}: {module}" for path, module in offenders)
        raise AssertionError(f"Forbidden imports found in products/:\n{details}")

# tests/unit/test_core_agent_roles.py
# ==============================
# Tests: Core role agents
# ==============================
from __future__ import annotations

import json

from core.agents.llm_reasoner import (
    ExplanationReasoner,
    InsightReasoner,
    PrioritizationReasoner,
)
from core.agents.registry import AgentRegistry
from core.contracts.agent_schema import AgentMeta, AgentResult, AgentKind
from core.contracts.flow_schema import StepDef, StepType
from core.orchestrator.context import RunContext


def _make_step_ctx(agent_name: str, params: dict) -> "object":
    step_def = StepDef(id="step", type=StepType.AGENT, agent=agent_name, params=params)
    run_ctx = RunContext(run_id="run_1", product="demo", flow="demo", payload={})
    return run_ctx.new_step(step_def=step_def)


def _fake_llm_result(payload: dict) -> AgentResult:
    meta = AgentMeta(agent_name="llm_reasoner", kind=AgentKind.OTHER, tags={})
    return AgentResult(ok=True, data={"content": json.dumps(payload)}, error=None, meta=meta)


def test_registry_registers_core_role_agents() -> None:
    AgentRegistry.clear()
    assert AgentRegistry.has("insight_reasoner")
    assert AgentRegistry.has("prioritization_reasoner")
    assert AgentRegistry.has("explanation_reasoner")


def test_insight_reasoner_parses_output(monkeypatch) -> None:
    payload = {"summary": "ok", "highlights": ["a"], "risks": []}
    monkeypatch.setattr(
        "core.agents.llm_reasoner.LlmReasoner.run",
        lambda self, ctx: _fake_llm_result(payload),
    )
    agent = InsightReasoner()
    result = agent.run(_make_step_ctx(agent.name, {"prompt": "test"}))
    assert result.ok
    assert result.data["summary"] == "ok"


def test_prioritization_reasoner_parses_output(monkeypatch) -> None:
    payload = {"priorities": [{"item": "A", "priority": 1, "rationale": "top"}]}
    monkeypatch.setattr(
        "core.agents.llm_reasoner.LlmReasoner.run",
        lambda self, ctx: _fake_llm_result(payload),
    )
    agent = PrioritizationReasoner()
    result = agent.run(_make_step_ctx(agent.name, {"prompt": "test"}))
    assert result.ok
    assert result.data["priorities"][0]["item"] == "A"


def test_explanation_reasoner_parses_output(monkeypatch) -> None:
    payload = {"explanation": "because", "assumptions": [], "limitations": []}
    monkeypatch.setattr(
        "core.agents.llm_reasoner.LlmReasoner.run",
        lambda self, ctx: _fake_llm_result(payload),
    )
    agent = ExplanationReasoner()
    result = agent.run(_make_step_ctx(agent.name, {"prompt": "test"}))
    assert result.ok
    assert result.data["explanation"] == "because"

# tests/unit/test_flow_no_subflow.py
# ==============================
# Tests: Disallow subflow steps in v1
# ==============================
from __future__ import annotations

import pytest

from core.contracts.flow_schema import FlowDef


def test_flow_subflow_is_rejected() -> None:
    with pytest.raises(Exception):
        FlowDef.model_validate(
            {
                "id": "demo",
                "version": "1.0.0",
                "steps": [
                    {
                        "id": "call_flow",
                        "type": "subflow",
                        "subflow": "other_flow",
                    }
                ],
            }
        )

# tests/unit/test_flow_user_input_schema.py
# ==============================
# Tests: Flow schema user_input steps
# ==============================
from __future__ import annotations

import pytest

from core.contracts.flow_schema import FlowDef


def test_flow_with_user_input_parses() -> None:
    flow = FlowDef.model_validate(
        {
            "id": "demo",
            "version": "1.0.0",
            "steps": [
                {
                    "id": "input",
                    "type": "user_input",
                    "params": {
                        "schema_version": "1.0",
                        "form_id": "demo_input",
                        "prompt": "Choose an option",
                        "input_type": "select",
                        "choices": [{"label": "A", "value": "a"}],
                        "defaults": {"choice": "a"},
                        "schema": {"type": "object", "properties": {"choice": {"type": "string"}}},
                    },
                }
            ],
        }
    )
    assert flow.id == "demo"
    assert flow.steps[0].type.value == "user_input"


def test_flow_user_input_missing_prompt_fails() -> None:
    with pytest.raises(Exception):
        FlowDef.model_validate(
            {
                "id": "demo",
                "version": "1.0.0",
                "steps": [
                    {
                        "id": "input",
                        "type": "user_input",
                        "params": {
                            "schema_version": "1.0",
                            "form_id": "demo_input",
                            "input_type": "text",
                        },
                    }
                ],
            }
        )

# tests/unit/test_orchestrator_no_product_imports.py
# ==============================
# Tests: Orchestrator must not import products
# ==============================
from __future__ import annotations

import ast
from pathlib import Path
from typing import Iterable, List, Tuple


FORBIDDEN_PREFIXES = ("products.",)


def _iter_python_files(root: Path) -> Iterable[Path]:
    for path in root.rglob("*.py"):
        if path.is_file():
            yield path


def _check_file(path: Path) -> List[Tuple[str, str]]:
    source = path.read_text(encoding="utf-8")
    tree = ast.parse(source, filename=str(path))
    offenders: List[Tuple[str, str]] = []
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                name = alias.name
                if any(name.startswith(prefix) for prefix in FORBIDDEN_PREFIXES):
                    offenders.append((str(path), name))
        elif isinstance(node, ast.ImportFrom):
            module = node.module
            if module and any(module.startswith(prefix) for prefix in FORBIDDEN_PREFIXES):
                offenders.append((str(path), module))
    return offenders


def test_orchestrator_does_not_import_products() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    orchestrator_root = repo_root / "core" / "orchestrator"
    offenders: List[Tuple[str, str]] = []
    for path in _iter_python_files(orchestrator_root):
        offenders.extend(_check_file(path))
    if offenders:
        details = "\n".join(f"{path}: {module}" for path, module in offenders)
        raise AssertionError(f"Forbidden product imports found in core/orchestrator:\n{details}")

# tests/unit/test_prompt_renderer.py
# ==============================
# Prompt Renderer Tests
# ==============================
from __future__ import annotations

import pytest

from core.orchestrator.templating import render_messages, render_template


def test_render_template_resolves_artifacts() -> None:
    context = {
        "artifacts": {
            "tool.data_reader.output": {"summary": "read 2 rows"},
        },
        "payload": {},
    }
    rendered = render_template(
        "Dataset: {{artifacts.tool.data_reader.output.summary}}",
        context,
    )
    assert rendered == "Dataset: read 2 rows"


def test_render_template_missing_placeholder_raises() -> None:
    context = {"artifacts": {}, "payload": {}}
    with pytest.raises(KeyError):
        render_template("Value: {{artifacts.tool.unknown.output}}", context)


def test_render_messages_handles_list() -> None:
    context = {
        "artifacts": {
            "tool.data_reader.output": {"summary": "ok"},
        },
        "payload": {"prompt": "hi"},
    }
    messages = [
        {"role": "system", "content": "System {{payload.prompt}}"},
        {"role": "user", "content": "Dataset {{artifacts.tool.data_reader.output.summary}}"},
    ]
    rendered = render_messages(messages, context)
    assert rendered[0]["content"] == "System hi"
    assert rendered[1]["content"] == "Dataset ok"

# tests/unit/test_sqlite_backend_concurrency.py
from __future__ import annotations

import threading

from core.contracts.run_schema import RunRecord
from core.memory.sqlite_backend import SQLiteBackend


def test_sqlite_backend_concurrent_writes(tmp_path) -> None:
    db_path = tmp_path / "runs.sqlite3"
    backend = SQLiteBackend(db_path=str(db_path), initialize=True)

    thread_count = 6
    runs_per_thread = 10
    total_runs = thread_count * runs_per_thread
    barrier = threading.Barrier(thread_count)
    errors = []
    lock = threading.Lock()

    def worker(thread_id: int) -> None:
        try:
            barrier.wait()
            for idx in range(runs_per_thread):
                run = RunRecord(
                    run_id=f"run-{thread_id}-{idx}",
                    product="demo",
                    flow="flow",
                    autonomy_level="semi_auto",
                )
                backend.create_run(run)
        except Exception as exc:  # pragma: no cover - failure path
            with lock:
                errors.append(exc)

    threads = [threading.Thread(target=worker, args=(i,)) for i in range(thread_count)]
    for thread in threads:
        thread.start()
    for thread in threads:
        thread.join()

    assert not errors, f"Concurrent writes raised errors: {errors}"

    runs = backend.list_runs(limit=total_runs + 5)
    run_ids = {run.run_id for run in runs}
    expected_ids = {f"run-{thread_id}-{idx}" for thread_id in range(thread_count) for idx in range(runs_per_thread)}
    assert run_ids == expected_ids


def test_sqlite_backend_uses_wal(tmp_path) -> None:
    db_path = tmp_path / "wal.sqlite3"
    backend = SQLiteBackend(db_path=str(db_path), initialize=True)
    con = backend._connect()
    try:
        journal_mode = con.execute("PRAGMA journal_mode;").fetchone()[0]
        busy_timeout = con.execute("PRAGMA busy_timeout;").fetchone()[0]
        assert str(journal_mode).lower() == "wal"
        assert int(busy_timeout) > 0
    finally:
        con.close()

# tests/unit/test_tool_no_llm_imports.py
# ==============================
# Tests: Tools must not import LLM providers
# ==============================
from __future__ import annotations

import ast
from pathlib import Path
from typing import Iterable, List, Tuple


FORBIDDEN_PREFIXES = (
    "core.agents.llm_reasoner",
    "core.models.router",
    "core.models.providers",
    "openai",
)


def _iter_python_files(root: Path) -> Iterable[Path]:
    for path in root.rglob("*.py"):
        if path.is_file():
            yield path


def _check_file(path: Path) -> List[Tuple[str, str]]:
    source = path.read_text(encoding="utf-8")
    tree = ast.parse(source, filename=str(path))
    offenders: List[Tuple[str, str]] = []
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                name = alias.name
                if any(name.startswith(prefix) for prefix in FORBIDDEN_PREFIXES):
                    offenders.append((str(path), name))
        elif isinstance(node, ast.ImportFrom):
            module = node.module
            if module and any(module.startswith(prefix) for prefix in FORBIDDEN_PREFIXES):
                offenders.append((str(path), module))
    return offenders


def test_tools_do_not_import_llm_providers() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    tool_roots = [
        repo_root / "core" / "tools",
        repo_root / "products",
    ]
    offenders: List[Tuple[str, str]] = []
    for root in tool_roots:
        if not root.exists():
            continue
        for path in _iter_python_files(root):
            if "tools" not in path.parts:
                continue
            offenders.extend(_check_file(path))
    if offenders:
        details = "\n".join(f"{path}: {module}" for path, module in offenders)
        raise AssertionError(f"Forbidden LLM imports found in tool code:\n{details}")

# tests/unit/test_v1_invariants.py
# ==============================
# Tests: V1 Invariants (Consolidated)
# ==============================
from __future__ import annotations

import ast
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

from tests.unit import test_agents_no_memory_backend_imports as agent_mem_guard
from tests.unit import test_architecture_guardrails as product_guard
from tests.unit import test_orchestrator_no_product_imports as orchestrator_guard
from tests.unit import test_tool_no_llm_imports as tool_llm_guard


_EXCLUDED_DIRS = {".git", ".venv", "venv", "__pycache__", "storage", "secrets", "tests"}


def _iter_python_files(root: Path) -> Iterable[Path]:
    for path in root.rglob("*.py"):
        if not path.is_file():
            continue
        if any(part in _EXCLUDED_DIRS for part in path.parts):
            continue
        yield path


def _format_report(sections: Dict[str, List[str]]) -> str:
    lines: List[str] = ["V1 invariant violations:"]
    for section, items in sections.items():
        if not items:
            continue
        lines.append(f"\n== {section} ==")
        lines.extend(f"- {item}" for item in sorted(items))
    return "\n".join(lines)


def _dedupe_sections(sections: Dict[str, List[str]]) -> Dict[str, List[str]]:
    seen = set()
    out: Dict[str, List[str]] = {}
    for section, items in sections.items():
        filtered: List[str] = []
        for item in items:
            if item in seen:
                continue
            seen.add(item)
            filtered.append(item)
        out[section] = filtered
    return out


def _scan_openai_imports(repo_root: Path) -> List[str]:
    offenders: List[str] = []
    providers_root = repo_root / "core" / "models" / "providers"
    for path in _iter_python_files(repo_root):
        if providers_root in path.parents:
            continue
        source = path.read_text(encoding="utf-8")
        tree = ast.parse(source, filename=str(path))
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias.name.startswith("openai"):
                        offenders.append(f"{path}: {alias.name}")
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                if module.startswith("openai"):
                    offenders.append(f"{path}: {module}")
    return offenders


def _scan_products_boundary(repo_root: Path) -> List[str]:
    offenders: List[str] = []
    products_root = repo_root / "products"
    for path in product_guard._iter_python_files(products_root):
        offenders.extend(f"{p}: {m}" for p, m in product_guard._check_file(path))
    return offenders


def _scan_tools_boundary(repo_root: Path) -> List[str]:
    offenders: List[str] = []
    tool_roots = [repo_root / "core" / "tools", repo_root / "products"]
    for root in tool_roots:
        if not root.exists():
            continue
        for path in tool_llm_guard._iter_python_files(root):
            if "tools" not in path.parts:
                continue
            offenders.extend(f"{p}: {m}" for p, m in tool_llm_guard._check_file(path))
    return offenders


def _scan_agents_boundary(repo_root: Path) -> List[str]:
    offenders: List[str] = []
    agent_roots = [repo_root / "core" / "agents", repo_root / "products"]
    for root in agent_roots:
        if not root.exists():
            continue
        for path in agent_mem_guard._iter_python_files(root):
            if "agents" not in path.parts:
                continue
            offenders.extend(f"{p}: {m}" for p, m in agent_mem_guard._check_file(path))
    return offenders


def _scan_orchestrator_boundary(repo_root: Path) -> List[str]:
    offenders: List[str] = []
    orchestrator_root = repo_root / "core" / "orchestrator"
    for path in orchestrator_guard._iter_python_files(orchestrator_root):
        offenders.extend(f"{p}: {m}" for p, m in orchestrator_guard._check_file(path))
    return offenders


def test_v1_invariants() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    sections = _dedupe_sections(
        {
        "Imports boundary checks (products)": _scan_products_boundary(repo_root),
        "Imports boundary checks (tools)": _scan_tools_boundary(repo_root),
        "Imports boundary checks (agents)": _scan_agents_boundary(repo_root),
        "Imports boundary checks (orchestrator)": _scan_orchestrator_boundary(repo_root),
        "Forbidden vendor SDK usage outside core/models/providers": _scan_openai_imports(repo_root),
        "Forbidden persistence outside core/memory (agents/products)": _scan_agents_boundary(repo_root),
        }
    )
    violations = [item for items in sections.values() for item in items]
    if violations:
        raise AssertionError(_format_report(sections))

# tests/unit/test_v1_negative_guardrails.py
# ==============================
# Tests: V1 Negative Guardrails
# ==============================
from __future__ import annotations

import ast
from pathlib import Path
from typing import Iterable, List, Tuple


_EXCLUDED_DIRS = {".git", ".venv", "venv", "__pycache__", "storage", "secrets", "tests"}


def _iter_python_files(root: Path) -> Iterable[Path]:
    for path in root.rglob("*.py"):
        if not path.is_file():
            continue
        if any(part in _EXCLUDED_DIRS for part in path.parts):
            continue
        yield path


def _scan_agent_to_agent_calls(root: Path) -> List[str]:
    offenders: List[str] = []
    for path in _iter_python_files(root):
        if "agents" not in path.parts:
            continue
        source = path.read_text(encoding="utf-8")
        tree = ast.parse(source, filename=str(path))
        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom) and node.module == "core.agents.registry":
                offenders.append(f"{path}: imports AgentRegistry")
            if isinstance(node, ast.Call):
                func = node.func
                if isinstance(func, ast.Attribute) and func.attr == "resolve":
                    if isinstance(func.value, ast.Name) and func.value.id == "AgentRegistry":
                        offenders.append(f"{path}: AgentRegistry.resolve()")
    return offenders


def _scan_dynamic_flow_mutation(root: Path) -> List[str]:
    offenders: List[str] = []
    for path in _iter_python_files(root):
        if "orchestrator" not in path.parts and "products" not in path.parts:
            continue
        source = path.read_text(encoding="utf-8")
        tree = ast.parse(source, filename=str(path))
        for node in ast.walk(tree):
            if isinstance(node, (ast.Assign, ast.AugAssign)) and isinstance(node.target if isinstance(node, ast.AugAssign) else node.targets[0], ast.Attribute):
                target = node.target if isinstance(node, ast.AugAssign) else node.targets[0]
                if isinstance(target, ast.Attribute) and target.attr == "steps":
                    offenders.append(f"{path}: assigns to steps")
            if isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):
                if isinstance(node.func.value, ast.Attribute) and node.func.value.attr == "steps":
                    if node.func.attr in {"append", "extend", "insert", "pop", "remove", "clear"}:
                        offenders.append(f"{path}: mutates steps via {node.func.attr}")
    return offenders


def _scan_autonomous_retries(root: Path) -> List[str]:
    offenders: List[str] = []
    for path in _iter_python_files(root):
        if not any(part in path.parts for part in ("agents", "tools", "products")):
            continue
        if path.name in {"step_executor.py", "error_policy.py"}:
            continue
        source = path.read_text(encoding="utf-8")
        tree = ast.parse(source, filename=str(path))
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias.name.startswith("tenacity"):
                        offenders.append(f"{path}: imports tenacity")
            if isinstance(node, ast.ImportFrom):
                if (node.module or "").startswith("tenacity"):
                    offenders.append(f"{path}: imports tenacity")
            if isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):
                if isinstance(node.func.value, ast.Name) and node.func.value.id in {"time", "asyncio"}:
                    if node.func.attr == "sleep":
                        offenders.append(f"{path}: sleep() usage")
    return offenders


def _scan_hidden_product_state(root: Path) -> List[str]:
    offenders: List[str] = []
    for path in _iter_python_files(root):
        if "products" not in path.parts:
            continue
        source = path.read_text(encoding="utf-8")
        tree = ast.parse(source, filename=str(path))
        for node in tree.body:
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if not isinstance(target, ast.Name):
                        continue
                    name = target.id
                    if name.isupper():
                        continue
                    if isinstance(node.value, (ast.Dict, ast.List, ast.Set)):
                        offenders.append(f"{path}: mutable module state '{name}'")
                    if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):
                        if node.value.func.id in {"dict", "list", "set"}:
                            offenders.append(f"{path}: mutable module state '{name}'")
    return offenders


def _scan_self_modifying_flows(root: Path) -> List[str]:
    offenders: List[str] = []
    for path in _iter_python_files(root):
        source = path.read_text(encoding="utf-8")
        tree = ast.parse(source, filename=str(path))
        for node in ast.walk(tree):
            if isinstance(node, ast.Call) and isinstance(node.func, ast.Attribute):
                if node.func.attr in {"write_text", "write_bytes"} and node.args:
                    arg = node.args[0]
                    if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                        if "/flows/" in arg.value and arg.value.endswith((".yaml", ".yml")):
                            offenders.append(f"{path}: writes flow file {arg.value}")
            if isinstance(node, ast.Call) and isinstance(node.func, ast.Name) and node.func.id == "open":
                if node.args:
                    arg = node.args[0]
                    if isinstance(arg, ast.Constant) and isinstance(arg.value, str):
                        if "/flows/" in arg.value and arg.value.endswith((".yaml", ".yml")):
                            offenders.append(f"{path}: opens flow file {arg.value}")
    return offenders


def _fail_if(offenders: List[str], *, title: str) -> None:
    if offenders:
        details = "\n".join(sorted(offenders))
        raise AssertionError(f"{title}:\n{details}")


def test_no_agent_to_agent_calls() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    offenders = _scan_agent_to_agent_calls(repo_root)
    _fail_if(offenders, title="Agent-to-agent calls are forbidden")


def test_no_dynamic_flow_mutation() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    offenders = _scan_dynamic_flow_mutation(repo_root)
    _fail_if(offenders, title="Dynamic flow mutation is forbidden")


def test_no_autonomous_retries_without_policy() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    offenders = _scan_autonomous_retries(repo_root)
    _fail_if(offenders, title="Autonomous retries without policy are forbidden")


def test_no_hidden_state_inside_products() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    offenders = _scan_hidden_product_state(repo_root)
    _fail_if(offenders, title="Hidden mutable state inside products is forbidden")


def test_no_self_modifying_flows() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    offenders = _scan_self_modifying_flows(repo_root)
    _fail_if(offenders, title="Self-modifying flow files are forbidden")
